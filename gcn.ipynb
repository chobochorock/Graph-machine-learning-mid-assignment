{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a29ae1b0",
   "metadata": {},
   "source": [
    "### 1. 두 가지의 symmetric normalization을 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6910e0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Epoch: 001, Loss: 1.9448, Train: 0.3214, Val: 0.2420, Test: 0.2420\n",
      "Epoch: 002, Loss: 1.9401, Train: 0.2857, Val: 0.1800, Test: 0.2420\n",
      "Epoch: 003, Loss: 1.9339, Train: 0.3929, Val: 0.3040, Test: 0.2780\n",
      "Epoch: 004, Loss: 1.9272, Train: 0.4643, Val: 0.3660, Test: 0.3460\n",
      "Epoch: 005, Loss: 1.9185, Train: 0.4714, Val: 0.3800, Test: 0.3580\n",
      "Epoch: 006, Loss: 1.9110, Train: 0.5500, Val: 0.4220, Test: 0.3930\n",
      "Epoch: 007, Loss: 1.9037, Train: 0.5857, Val: 0.4340, Test: 0.4180\n",
      "Epoch: 008, Loss: 1.8928, Train: 0.6643, Val: 0.4800, Test: 0.4740\n",
      "Epoch: 009, Loss: 1.8840, Train: 0.6571, Val: 0.4840, Test: 0.4900\n",
      "Epoch: 010, Loss: 1.8699, Train: 0.6643, Val: 0.4780, Test: 0.4900\n",
      "Epoch: 011, Loss: 1.8621, Train: 0.6714, Val: 0.4500, Test: 0.4900\n",
      "Epoch: 012, Loss: 1.8496, Train: 0.6643, Val: 0.4620, Test: 0.4900\n",
      "Epoch: 013, Loss: 1.8476, Train: 0.6714, Val: 0.4580, Test: 0.4900\n",
      "Epoch: 014, Loss: 1.8354, Train: 0.6786, Val: 0.4820, Test: 0.4900\n",
      "Epoch: 015, Loss: 1.8188, Train: 0.7429, Val: 0.5540, Test: 0.5530\n",
      "Epoch: 016, Loss: 1.8059, Train: 0.8143, Val: 0.6180, Test: 0.6240\n",
      "Epoch: 017, Loss: 1.7976, Train: 0.8357, Val: 0.6600, Test: 0.6530\n",
      "Epoch: 018, Loss: 1.7739, Train: 0.8357, Val: 0.6860, Test: 0.6710\n",
      "Epoch: 019, Loss: 1.7645, Train: 0.8357, Val: 0.6760, Test: 0.6710\n",
      "Epoch: 020, Loss: 1.7539, Train: 0.8429, Val: 0.6940, Test: 0.6600\n",
      "Epoch: 021, Loss: 1.7472, Train: 0.8429, Val: 0.6960, Test: 0.6680\n",
      "Epoch: 022, Loss: 1.7350, Train: 0.8500, Val: 0.6940, Test: 0.6680\n",
      "Epoch: 023, Loss: 1.7103, Train: 0.8643, Val: 0.7080, Test: 0.6950\n",
      "Epoch: 024, Loss: 1.6845, Train: 0.8714, Val: 0.7220, Test: 0.7020\n",
      "Epoch: 025, Loss: 1.6808, Train: 0.8786, Val: 0.7300, Test: 0.7140\n",
      "Epoch: 026, Loss: 1.6672, Train: 0.8714, Val: 0.7400, Test: 0.7290\n",
      "Epoch: 027, Loss: 1.6600, Train: 0.8786, Val: 0.7460, Test: 0.7380\n",
      "Epoch: 028, Loss: 1.6372, Train: 0.8786, Val: 0.7500, Test: 0.7520\n",
      "Epoch: 029, Loss: 1.6238, Train: 0.8786, Val: 0.7540, Test: 0.7580\n",
      "Epoch: 030, Loss: 1.5957, Train: 0.8786, Val: 0.7580, Test: 0.7590\n",
      "Epoch: 031, Loss: 1.5832, Train: 0.8786, Val: 0.7600, Test: 0.7590\n",
      "Epoch: 032, Loss: 1.5830, Train: 0.8786, Val: 0.7520, Test: 0.7590\n",
      "Epoch: 033, Loss: 1.5311, Train: 0.8714, Val: 0.7480, Test: 0.7590\n",
      "Epoch: 034, Loss: 1.5280, Train: 0.8714, Val: 0.7380, Test: 0.7590\n",
      "Epoch: 035, Loss: 1.5120, Train: 0.8786, Val: 0.7480, Test: 0.7590\n",
      "Epoch: 036, Loss: 1.4811, Train: 0.8786, Val: 0.7500, Test: 0.7590\n",
      "Epoch: 037, Loss: 1.4598, Train: 0.8857, Val: 0.7500, Test: 0.7590\n",
      "Epoch: 038, Loss: 1.4356, Train: 0.8857, Val: 0.7540, Test: 0.7590\n",
      "Epoch: 039, Loss: 1.4391, Train: 0.8786, Val: 0.7560, Test: 0.7590\n",
      "Epoch: 040, Loss: 1.4324, Train: 0.8786, Val: 0.7640, Test: 0.7560\n",
      "Epoch: 041, Loss: 1.4102, Train: 0.8786, Val: 0.7680, Test: 0.7640\n",
      "Epoch: 042, Loss: 1.3991, Train: 0.8786, Val: 0.7680, Test: 0.7640\n",
      "Epoch: 043, Loss: 1.3344, Train: 0.8857, Val: 0.7720, Test: 0.7650\n",
      "Epoch: 044, Loss: 1.3416, Train: 0.8857, Val: 0.7740, Test: 0.7680\n",
      "Epoch: 045, Loss: 1.3193, Train: 0.8786, Val: 0.7740, Test: 0.7680\n",
      "Epoch: 046, Loss: 1.2985, Train: 0.8714, Val: 0.7740, Test: 0.7680\n",
      "Epoch: 047, Loss: 1.2770, Train: 0.8714, Val: 0.7680, Test: 0.7680\n",
      "Epoch: 048, Loss: 1.2561, Train: 0.8786, Val: 0.7660, Test: 0.7680\n",
      "Epoch: 049, Loss: 1.2778, Train: 0.8929, Val: 0.7660, Test: 0.7680\n",
      "Epoch: 050, Loss: 1.2317, Train: 0.8929, Val: 0.7680, Test: 0.7680\n",
      "Epoch: 051, Loss: 1.2468, Train: 0.8857, Val: 0.7700, Test: 0.7680\n",
      "Epoch: 052, Loss: 1.1839, Train: 0.8857, Val: 0.7820, Test: 0.7710\n",
      "Epoch: 053, Loss: 1.1756, Train: 0.8929, Val: 0.7820, Test: 0.7710\n",
      "Epoch: 054, Loss: 1.1358, Train: 0.9000, Val: 0.7860, Test: 0.7800\n",
      "Epoch: 055, Loss: 1.1380, Train: 0.9071, Val: 0.7900, Test: 0.7840\n",
      "Epoch: 056, Loss: 1.1011, Train: 0.9000, Val: 0.7920, Test: 0.7890\n",
      "Epoch: 057, Loss: 1.1238, Train: 0.9143, Val: 0.7920, Test: 0.7890\n",
      "Epoch: 058, Loss: 1.0827, Train: 0.9143, Val: 0.7940, Test: 0.7920\n",
      "Epoch: 059, Loss: 1.0880, Train: 0.9143, Val: 0.7980, Test: 0.7930\n",
      "Epoch: 060, Loss: 1.0628, Train: 0.9214, Val: 0.7960, Test: 0.7930\n",
      "Epoch: 061, Loss: 1.0479, Train: 0.9214, Val: 0.7960, Test: 0.7930\n",
      "Epoch: 062, Loss: 1.0227, Train: 0.9214, Val: 0.7940, Test: 0.7930\n",
      "Epoch: 063, Loss: 0.9890, Train: 0.9214, Val: 0.7920, Test: 0.7930\n",
      "Epoch: 064, Loss: 0.9597, Train: 0.9214, Val: 0.7960, Test: 0.7930\n",
      "Epoch: 065, Loss: 0.9284, Train: 0.9071, Val: 0.7940, Test: 0.7930\n",
      "Epoch: 066, Loss: 0.9712, Train: 0.9071, Val: 0.7920, Test: 0.7930\n",
      "Epoch: 067, Loss: 0.9118, Train: 0.9214, Val: 0.7900, Test: 0.7930\n",
      "Epoch: 068, Loss: 0.9201, Train: 0.9214, Val: 0.7940, Test: 0.7930\n",
      "Epoch: 069, Loss: 0.8961, Train: 0.9214, Val: 0.8000, Test: 0.7970\n",
      "Epoch: 070, Loss: 0.9119, Train: 0.9143, Val: 0.8040, Test: 0.7980\n",
      "Epoch: 071, Loss: 0.8634, Train: 0.9214, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 072, Loss: 0.8544, Train: 0.9214, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 073, Loss: 0.8815, Train: 0.9286, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 074, Loss: 0.8202, Train: 0.9286, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 075, Loss: 0.8785, Train: 0.9214, Val: 0.7960, Test: 0.7980\n",
      "Epoch: 076, Loss: 0.8335, Train: 0.9214, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 077, Loss: 0.8310, Train: 0.9286, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 078, Loss: 0.7990, Train: 0.9286, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 079, Loss: 0.7802, Train: 0.9214, Val: 0.7940, Test: 0.7980\n",
      "Epoch: 080, Loss: 0.7832, Train: 0.9214, Val: 0.7960, Test: 0.7980\n",
      "Epoch: 081, Loss: 0.7704, Train: 0.9214, Val: 0.7920, Test: 0.7980\n",
      "Epoch: 082, Loss: 0.7257, Train: 0.9286, Val: 0.7900, Test: 0.7980\n",
      "Epoch: 083, Loss: 0.8050, Train: 0.9286, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 084, Loss: 0.7445, Train: 0.9357, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 085, Loss: 0.7188, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 086, Loss: 0.6953, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 087, Loss: 0.7222, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 088, Loss: 0.7585, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 089, Loss: 0.6936, Train: 0.9357, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 090, Loss: 0.6962, Train: 0.9357, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 091, Loss: 0.7223, Train: 0.9357, Val: 0.7960, Test: 0.7980\n",
      "Epoch: 092, Loss: 0.6845, Train: 0.9357, Val: 0.7960, Test: 0.7980\n",
      "Epoch: 093, Loss: 0.6747, Train: 0.9357, Val: 0.7920, Test: 0.7980\n",
      "Epoch: 094, Loss: 0.6516, Train: 0.9357, Val: 0.7880, Test: 0.7980\n",
      "Epoch: 095, Loss: 0.6670, Train: 0.9286, Val: 0.7880, Test: 0.7980\n",
      "Epoch: 096, Loss: 0.6538, Train: 0.9286, Val: 0.7840, Test: 0.7980\n",
      "Epoch: 097, Loss: 0.6266, Train: 0.9286, Val: 0.7900, Test: 0.7980\n",
      "Epoch: 098, Loss: 0.6129, Train: 0.9214, Val: 0.7940, Test: 0.7980\n",
      "Epoch: 099, Loss: 0.6355, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 100, Loss: 0.6459, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 101, Loss: 0.5975, Train: 0.9286, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 102, Loss: 0.6280, Train: 0.9357, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 103, Loss: 0.6198, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 104, Loss: 0.6262, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 105, Loss: 0.5971, Train: 0.9286, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 106, Loss: 0.5831, Train: 0.9286, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 107, Loss: 0.5975, Train: 0.9214, Val: 0.7920, Test: 0.7980\n",
      "Epoch: 108, Loss: 0.5826, Train: 0.9286, Val: 0.7960, Test: 0.7980\n",
      "Epoch: 109, Loss: 0.5851, Train: 0.9286, Val: 0.7920, Test: 0.7980\n",
      "Epoch: 110, Loss: 0.6117, Train: 0.9286, Val: 0.7940, Test: 0.7980\n",
      "Epoch: 111, Loss: 0.5750, Train: 0.9286, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 112, Loss: 0.5719, Train: 0.9286, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 113, Loss: 0.5760, Train: 0.9357, Val: 0.8020, Test: 0.7980\n",
      "Epoch: 114, Loss: 0.5783, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 115, Loss: 0.5357, Train: 0.9429, Val: 0.8020, Test: 0.7980\n",
      "Epoch: 116, Loss: 0.5710, Train: 0.9429, Val: 0.8020, Test: 0.7980\n",
      "Epoch: 117, Loss: 0.5917, Train: 0.9429, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 118, Loss: 0.5295, Train: 0.9429, Val: 0.8020, Test: 0.7980\n",
      "Epoch: 119, Loss: 0.5354, Train: 0.9429, Val: 0.8020, Test: 0.7980\n",
      "Epoch: 120, Loss: 0.5454, Train: 0.9429, Val: 0.8040, Test: 0.7980\n",
      "Epoch: 121, Loss: 0.5590, Train: 0.9357, Val: 0.8080, Test: 0.8160\n",
      "Epoch: 122, Loss: 0.5490, Train: 0.9429, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 123, Loss: 0.5326, Train: 0.9429, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 124, Loss: 0.5141, Train: 0.9357, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 125, Loss: 0.5369, Train: 0.9357, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 126, Loss: 0.5191, Train: 0.9500, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 127, Loss: 0.5098, Train: 0.9500, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 128, Loss: 0.5415, Train: 0.9500, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 129, Loss: 0.5678, Train: 0.9429, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 130, Loss: 0.4888, Train: 0.9357, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 131, Loss: 0.5010, Train: 0.9429, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 132, Loss: 0.5276, Train: 0.9429, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 133, Loss: 0.5224, Train: 0.9500, Val: 0.8080, Test: 0.8160\n",
      "Epoch: 134, Loss: 0.5084, Train: 0.9429, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 135, Loss: 0.5157, Train: 0.9500, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 136, Loss: 0.5065, Train: 0.9571, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 137, Loss: 0.5092, Train: 0.9571, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 138, Loss: 0.5309, Train: 0.9571, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 139, Loss: 0.4815, Train: 0.9571, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 140, Loss: 0.4550, Train: 0.9429, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 141, Loss: 0.4929, Train: 0.9429, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 142, Loss: 0.4806, Train: 0.9500, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 143, Loss: 0.4687, Train: 0.9500, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 144, Loss: 0.4852, Train: 0.9500, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 145, Loss: 0.5223, Train: 0.9357, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 146, Loss: 0.4691, Train: 0.9357, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 147, Loss: 0.4417, Train: 0.9500, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 148, Loss: 0.4817, Train: 0.9500, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 149, Loss: 0.4648, Train: 0.9500, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 150, Loss: 0.4487, Train: 0.9429, Val: 0.8140, Test: 0.8150\n",
      "Epoch: 151, Loss: 0.4633, Train: 0.9429, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 152, Loss: 0.4551, Train: 0.9429, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 153, Loss: 0.4880, Train: 0.9429, Val: 0.8120, Test: 0.8150\n",
      "Epoch: 154, Loss: 0.4906, Train: 0.9429, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 155, Loss: 0.4243, Train: 0.9429, Val: 0.8120, Test: 0.8150\n",
      "Epoch: 156, Loss: 0.4476, Train: 0.9500, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 157, Loss: 0.4397, Train: 0.9571, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 158, Loss: 0.4351, Train: 0.9500, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 159, Loss: 0.4498, Train: 0.9500, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 160, Loss: 0.4428, Train: 0.9500, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 161, Loss: 0.4701, Train: 0.9571, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 162, Loss: 0.4576, Train: 0.9571, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 163, Loss: 0.4559, Train: 0.9571, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 164, Loss: 0.4075, Train: 0.9429, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 165, Loss: 0.4636, Train: 0.9429, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 166, Loss: 0.4474, Train: 0.9429, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 167, Loss: 0.4513, Train: 0.9571, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 168, Loss: 0.4699, Train: 0.9571, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 169, Loss: 0.4327, Train: 0.9643, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 170, Loss: 0.3922, Train: 0.9500, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 171, Loss: 0.4275, Train: 0.9500, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 172, Loss: 0.4583, Train: 0.9500, Val: 0.8120, Test: 0.8150\n",
      "Epoch: 173, Loss: 0.4414, Train: 0.9571, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 174, Loss: 0.4618, Train: 0.9571, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 175, Loss: 0.4302, Train: 0.9571, Val: 0.8120, Test: 0.8150\n",
      "Epoch: 176, Loss: 0.4525, Train: 0.9571, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 177, Loss: 0.4180, Train: 0.9571, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 178, Loss: 0.4479, Train: 0.9429, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 179, Loss: 0.4128, Train: 0.9571, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 180, Loss: 0.4075, Train: 0.9571, Val: 0.8140, Test: 0.8150\n",
      "Epoch: 181, Loss: 0.4128, Train: 0.9571, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 182, Loss: 0.4404, Train: 0.9571, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 183, Loss: 0.4347, Train: 0.9643, Val: 0.8120, Test: 0.8150\n",
      "Epoch: 184, Loss: 0.4327, Train: 0.9571, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 185, Loss: 0.4181, Train: 0.9643, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 186, Loss: 0.3968, Train: 0.9571, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 187, Loss: 0.4190, Train: 0.9571, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 188, Loss: 0.4291, Train: 0.9500, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 189, Loss: 0.4393, Train: 0.9500, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 190, Loss: 0.3795, Train: 0.9500, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 191, Loss: 0.4213, Train: 0.9571, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 192, Loss: 0.3590, Train: 0.9571, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 193, Loss: 0.4335, Train: 0.9571, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 194, Loss: 0.4138, Train: 0.9571, Val: 0.8120, Test: 0.8150\n",
      "Epoch: 195, Loss: 0.4218, Train: 0.9714, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 196, Loss: 0.4058, Train: 0.9714, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 197, Loss: 0.4480, Train: 0.9643, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 198, Loss: 0.3809, Train: 0.9714, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 199, Loss: 0.4310, Train: 0.9571, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 200, Loss: 0.3837, Train: 0.9500, Val: 0.8080, Test: 0.8150\n"
     ]
    }
   ],
   "source": [
    "# symmetric version (default)\n",
    "!python gcn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ed69e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9463, Train: 0.3071, Val: 0.2060, Test: 0.1980\n",
      "Epoch: 002, Loss: 1.9434, Train: 0.2214, Val: 0.1420, Test: 0.1980\n",
      "Epoch: 003, Loss: 1.9398, Train: 0.6643, Val: 0.6580, Test: 0.6530\n",
      "Epoch: 004, Loss: 1.9371, Train: 0.4357, Val: 0.4580, Test: 0.6530\n",
      "Epoch: 005, Loss: 1.9323, Train: 0.5143, Val: 0.5080, Test: 0.6530\n",
      "Epoch: 006, Loss: 1.9261, Train: 0.6429, Val: 0.5680, Test: 0.6530\n",
      "Epoch: 007, Loss: 1.9199, Train: 0.6857, Val: 0.6340, Test: 0.6530\n",
      "Epoch: 008, Loss: 1.9152, Train: 0.7286, Val: 0.6860, Test: 0.7130\n",
      "Epoch: 009, Loss: 1.9095, Train: 0.7143, Val: 0.6780, Test: 0.7130\n",
      "Epoch: 010, Loss: 1.9001, Train: 0.7286, Val: 0.6980, Test: 0.7200\n",
      "Epoch: 011, Loss: 1.8953, Train: 0.7786, Val: 0.7080, Test: 0.7320\n",
      "Epoch: 012, Loss: 1.8854, Train: 0.7786, Val: 0.6960, Test: 0.7320\n",
      "Epoch: 013, Loss: 1.8807, Train: 0.8000, Val: 0.7080, Test: 0.7320\n",
      "Epoch: 014, Loss: 1.8678, Train: 0.8429, Val: 0.7340, Test: 0.7160\n",
      "Epoch: 015, Loss: 1.8597, Train: 0.8643, Val: 0.7420, Test: 0.7330\n",
      "Epoch: 016, Loss: 1.8522, Train: 0.8429, Val: 0.7580, Test: 0.7530\n",
      "Epoch: 017, Loss: 1.8416, Train: 0.8429, Val: 0.7540, Test: 0.7530\n",
      "Epoch: 018, Loss: 1.8356, Train: 0.8357, Val: 0.7520, Test: 0.7530\n",
      "Epoch: 019, Loss: 1.8183, Train: 0.8357, Val: 0.7520, Test: 0.7530\n",
      "Epoch: 020, Loss: 1.8120, Train: 0.8143, Val: 0.7640, Test: 0.7800\n",
      "Epoch: 021, Loss: 1.8022, Train: 0.8000, Val: 0.7560, Test: 0.7800\n",
      "Epoch: 022, Loss: 1.7908, Train: 0.7929, Val: 0.7460, Test: 0.7800\n",
      "Epoch: 023, Loss: 1.7709, Train: 0.7714, Val: 0.7460, Test: 0.7800\n",
      "Epoch: 024, Loss: 1.7653, Train: 0.7857, Val: 0.7440, Test: 0.7800\n",
      "Epoch: 025, Loss: 1.7652, Train: 0.8071, Val: 0.7480, Test: 0.7800\n",
      "Epoch: 026, Loss: 1.7299, Train: 0.8000, Val: 0.7580, Test: 0.7800\n",
      "Epoch: 027, Loss: 1.7249, Train: 0.8000, Val: 0.7620, Test: 0.7800\n",
      "Epoch: 028, Loss: 1.7270, Train: 0.8000, Val: 0.7640, Test: 0.7800\n",
      "Epoch: 029, Loss: 1.7098, Train: 0.8000, Val: 0.7640, Test: 0.7800\n",
      "Epoch: 030, Loss: 1.6976, Train: 0.7929, Val: 0.7600, Test: 0.7800\n",
      "Epoch: 031, Loss: 1.6846, Train: 0.7786, Val: 0.7620, Test: 0.7800\n",
      "Epoch: 032, Loss: 1.6406, Train: 0.7786, Val: 0.7660, Test: 0.7700\n",
      "Epoch: 033, Loss: 1.6755, Train: 0.7857, Val: 0.7680, Test: 0.7720\n",
      "Epoch: 034, Loss: 1.6316, Train: 0.8000, Val: 0.7700, Test: 0.7750\n",
      "Epoch: 035, Loss: 1.6268, Train: 0.8071, Val: 0.7780, Test: 0.7840\n",
      "Epoch: 036, Loss: 1.6055, Train: 0.8143, Val: 0.7820, Test: 0.7890\n",
      "Epoch: 037, Loss: 1.5908, Train: 0.8214, Val: 0.7880, Test: 0.7840\n",
      "Epoch: 038, Loss: 1.5685, Train: 0.8214, Val: 0.7800, Test: 0.7840\n",
      "Epoch: 039, Loss: 1.5481, Train: 0.8143, Val: 0.7740, Test: 0.7840\n",
      "Epoch: 040, Loss: 1.5536, Train: 0.8143, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 041, Loss: 1.5306, Train: 0.8071, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 042, Loss: 1.4936, Train: 0.8000, Val: 0.7780, Test: 0.7840\n",
      "Epoch: 043, Loss: 1.4783, Train: 0.7929, Val: 0.7740, Test: 0.7840\n",
      "Epoch: 044, Loss: 1.4849, Train: 0.8000, Val: 0.7700, Test: 0.7840\n",
      "Epoch: 045, Loss: 1.4646, Train: 0.7857, Val: 0.7540, Test: 0.7840\n",
      "Epoch: 046, Loss: 1.4407, Train: 0.7786, Val: 0.7540, Test: 0.7840\n",
      "Epoch: 047, Loss: 1.4310, Train: 0.7857, Val: 0.7460, Test: 0.7840\n",
      "Epoch: 048, Loss: 1.4256, Train: 0.7857, Val: 0.7420, Test: 0.7840\n",
      "Epoch: 049, Loss: 1.3976, Train: 0.7929, Val: 0.7440, Test: 0.7840\n",
      "Epoch: 050, Loss: 1.3701, Train: 0.7929, Val: 0.7400, Test: 0.7840\n",
      "Epoch: 051, Loss: 1.3640, Train: 0.8000, Val: 0.7400, Test: 0.7840\n",
      "Epoch: 052, Loss: 1.3214, Train: 0.8143, Val: 0.7440, Test: 0.7840\n",
      "Epoch: 053, Loss: 1.3205, Train: 0.8214, Val: 0.7520, Test: 0.7840\n",
      "Epoch: 054, Loss: 1.3145, Train: 0.8143, Val: 0.7600, Test: 0.7840\n",
      "Epoch: 055, Loss: 1.2910, Train: 0.8071, Val: 0.7600, Test: 0.7840\n",
      "Epoch: 056, Loss: 1.2481, Train: 0.8000, Val: 0.7780, Test: 0.7840\n",
      "Epoch: 057, Loss: 1.2591, Train: 0.8000, Val: 0.7780, Test: 0.7840\n",
      "Epoch: 058, Loss: 1.2524, Train: 0.8000, Val: 0.7740, Test: 0.7840\n",
      "Epoch: 059, Loss: 1.1904, Train: 0.8000, Val: 0.7720, Test: 0.7840\n",
      "Epoch: 060, Loss: 1.2496, Train: 0.8000, Val: 0.7600, Test: 0.7840\n",
      "Epoch: 061, Loss: 1.1770, Train: 0.7929, Val: 0.7640, Test: 0.7840\n",
      "Epoch: 062, Loss: 1.1882, Train: 0.7929, Val: 0.7560, Test: 0.7840\n",
      "Epoch: 063, Loss: 1.1440, Train: 0.7929, Val: 0.7520, Test: 0.7840\n",
      "Epoch: 064, Loss: 1.1367, Train: 0.7857, Val: 0.7460, Test: 0.7840\n",
      "Epoch: 065, Loss: 1.1629, Train: 0.7929, Val: 0.7500, Test: 0.7840\n",
      "Epoch: 066, Loss: 1.1328, Train: 0.8071, Val: 0.7540, Test: 0.7840\n",
      "Epoch: 067, Loss: 1.0994, Train: 0.8000, Val: 0.7600, Test: 0.7840\n",
      "Epoch: 068, Loss: 1.0905, Train: 0.8071, Val: 0.7740, Test: 0.7840\n",
      "Epoch: 069, Loss: 1.0596, Train: 0.8143, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 070, Loss: 1.0821, Train: 0.8214, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 071, Loss: 1.0922, Train: 0.8286, Val: 0.7740, Test: 0.7840\n",
      "Epoch: 072, Loss: 1.1137, Train: 0.8357, Val: 0.7720, Test: 0.7840\n",
      "Epoch: 073, Loss: 1.0430, Train: 0.8286, Val: 0.7700, Test: 0.7840\n",
      "Epoch: 074, Loss: 0.9899, Train: 0.8286, Val: 0.7680, Test: 0.7840\n",
      "Epoch: 075, Loss: 1.0252, Train: 0.8214, Val: 0.7660, Test: 0.7840\n",
      "Epoch: 076, Loss: 0.9896, Train: 0.8143, Val: 0.7720, Test: 0.7840\n",
      "Epoch: 077, Loss: 0.9743, Train: 0.8000, Val: 0.7680, Test: 0.7840\n",
      "Epoch: 078, Loss: 0.9798, Train: 0.8071, Val: 0.7620, Test: 0.7840\n",
      "Epoch: 079, Loss: 0.9399, Train: 0.8071, Val: 0.7640, Test: 0.7840\n",
      "Epoch: 080, Loss: 0.9918, Train: 0.8143, Val: 0.7680, Test: 0.7840\n",
      "Epoch: 081, Loss: 0.9402, Train: 0.8286, Val: 0.7700, Test: 0.7840\n",
      "Epoch: 082, Loss: 0.9183, Train: 0.8286, Val: 0.7640, Test: 0.7840\n",
      "Epoch: 083, Loss: 0.9224, Train: 0.8286, Val: 0.7680, Test: 0.7840\n",
      "Epoch: 084, Loss: 0.9421, Train: 0.8286, Val: 0.7800, Test: 0.7840\n",
      "Epoch: 085, Loss: 0.9108, Train: 0.8214, Val: 0.7780, Test: 0.7840\n",
      "Epoch: 086, Loss: 0.8824, Train: 0.8143, Val: 0.7720, Test: 0.7840\n",
      "Epoch: 087, Loss: 0.9202, Train: 0.8357, Val: 0.7720, Test: 0.7840\n",
      "Epoch: 088, Loss: 0.8777, Train: 0.8429, Val: 0.7760, Test: 0.7840\n",
      "Epoch: 089, Loss: 0.8789, Train: 0.8429, Val: 0.7780, Test: 0.7840\n",
      "Epoch: 090, Loss: 0.8493, Train: 0.8500, Val: 0.7780, Test: 0.7840\n",
      "Epoch: 091, Loss: 0.8237, Train: 0.8500, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 092, Loss: 0.8390, Train: 0.8357, Val: 0.7860, Test: 0.7840\n",
      "Epoch: 093, Loss: 0.8282, Train: 0.8429, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 094, Loss: 0.8328, Train: 0.8571, Val: 0.7780, Test: 0.7840\n",
      "Epoch: 095, Loss: 0.8546, Train: 0.8571, Val: 0.7700, Test: 0.7840\n",
      "Epoch: 096, Loss: 0.8241, Train: 0.8429, Val: 0.7700, Test: 0.7840\n",
      "Epoch: 097, Loss: 0.8655, Train: 0.8429, Val: 0.7680, Test: 0.7840\n",
      "Epoch: 098, Loss: 0.8238, Train: 0.8429, Val: 0.7640, Test: 0.7840\n",
      "Epoch: 099, Loss: 0.8281, Train: 0.8429, Val: 0.7640, Test: 0.7840\n",
      "Epoch: 100, Loss: 0.7837, Train: 0.8500, Val: 0.7640, Test: 0.7840\n",
      "Epoch: 101, Loss: 0.7755, Train: 0.8500, Val: 0.7680, Test: 0.7840\n",
      "Epoch: 102, Loss: 0.7841, Train: 0.8500, Val: 0.7660, Test: 0.7840\n",
      "Epoch: 103, Loss: 0.8112, Train: 0.8643, Val: 0.7760, Test: 0.7840\n",
      "Epoch: 104, Loss: 0.7735, Train: 0.8714, Val: 0.7800, Test: 0.7840\n",
      "Epoch: 105, Loss: 0.8002, Train: 0.8714, Val: 0.7720, Test: 0.7840\n",
      "Epoch: 106, Loss: 0.7788, Train: 0.8714, Val: 0.7740, Test: 0.7840\n",
      "Epoch: 107, Loss: 0.7723, Train: 0.8786, Val: 0.7700, Test: 0.7840\n",
      "Epoch: 108, Loss: 0.7760, Train: 0.8786, Val: 0.7700, Test: 0.7840\n",
      "Epoch: 109, Loss: 0.7622, Train: 0.8714, Val: 0.7800, Test: 0.7840\n",
      "Epoch: 110, Loss: 0.7862, Train: 0.8714, Val: 0.7800, Test: 0.7840\n",
      "Epoch: 111, Loss: 0.7308, Train: 0.8714, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 112, Loss: 0.7423, Train: 0.8929, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 113, Loss: 0.6975, Train: 0.8929, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 114, Loss: 0.7206, Train: 0.8786, Val: 0.7800, Test: 0.7840\n",
      "Epoch: 115, Loss: 0.6845, Train: 0.8714, Val: 0.7800, Test: 0.7840\n",
      "Epoch: 116, Loss: 0.6860, Train: 0.8643, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 117, Loss: 0.7254, Train: 0.8714, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 118, Loss: 0.7020, Train: 0.8714, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 119, Loss: 0.6790, Train: 0.8643, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 120, Loss: 0.6855, Train: 0.8643, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 121, Loss: 0.6710, Train: 0.8714, Val: 0.7800, Test: 0.7840\n",
      "Epoch: 122, Loss: 0.6824, Train: 0.8857, Val: 0.7800, Test: 0.7840\n",
      "Epoch: 123, Loss: 0.7092, Train: 0.8929, Val: 0.7720, Test: 0.7840\n",
      "Epoch: 124, Loss: 0.6854, Train: 0.9071, Val: 0.7740, Test: 0.7840\n",
      "Epoch: 125, Loss: 0.6745, Train: 0.9071, Val: 0.7800, Test: 0.7840\n",
      "Epoch: 126, Loss: 0.6942, Train: 0.9000, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 127, Loss: 0.7006, Train: 0.9000, Val: 0.7860, Test: 0.7840\n",
      "Epoch: 128, Loss: 0.6479, Train: 0.9000, Val: 0.7880, Test: 0.7840\n",
      "Epoch: 129, Loss: 0.6583, Train: 0.9071, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 130, Loss: 0.6281, Train: 0.9000, Val: 0.7760, Test: 0.7840\n",
      "Epoch: 131, Loss: 0.6808, Train: 0.8857, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 132, Loss: 0.6476, Train: 0.8857, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 133, Loss: 0.6571, Train: 0.8857, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 134, Loss: 0.6531, Train: 0.8929, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 135, Loss: 0.6538, Train: 0.9071, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 136, Loss: 0.6632, Train: 0.9071, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 137, Loss: 0.6645, Train: 0.9000, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 138, Loss: 0.6159, Train: 0.8929, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 139, Loss: 0.6339, Train: 0.8929, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 140, Loss: 0.6192, Train: 0.9071, Val: 0.7800, Test: 0.7840\n",
      "Epoch: 141, Loss: 0.6116, Train: 0.9071, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 142, Loss: 0.5886, Train: 0.9071, Val: 0.7800, Test: 0.7840\n",
      "Epoch: 143, Loss: 0.6435, Train: 0.9071, Val: 0.7780, Test: 0.7840\n",
      "Epoch: 144, Loss: 0.5956, Train: 0.8929, Val: 0.7800, Test: 0.7840\n",
      "Epoch: 145, Loss: 0.6413, Train: 0.8929, Val: 0.7780, Test: 0.7840\n",
      "Epoch: 146, Loss: 0.5960, Train: 0.9000, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 147, Loss: 0.6081, Train: 0.9000, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 148, Loss: 0.6154, Train: 0.9143, Val: 0.7880, Test: 0.7840\n",
      "Epoch: 149, Loss: 0.6059, Train: 0.9143, Val: 0.7900, Test: 0.8020\n",
      "Epoch: 150, Loss: 0.6004, Train: 0.9143, Val: 0.7880, Test: 0.8020\n",
      "Epoch: 151, Loss: 0.6386, Train: 0.9143, Val: 0.7920, Test: 0.8070\n",
      "Epoch: 152, Loss: 0.6160, Train: 0.9071, Val: 0.7960, Test: 0.8020\n",
      "Epoch: 153, Loss: 0.5458, Train: 0.9071, Val: 0.7960, Test: 0.8020\n",
      "Epoch: 154, Loss: 0.5956, Train: 0.9071, Val: 0.7940, Test: 0.8020\n",
      "Epoch: 155, Loss: 0.5647, Train: 0.9071, Val: 0.7920, Test: 0.8020\n",
      "Epoch: 156, Loss: 0.5406, Train: 0.9071, Val: 0.7900, Test: 0.8020\n",
      "Epoch: 157, Loss: 0.5549, Train: 0.9071, Val: 0.7880, Test: 0.8020\n",
      "Epoch: 158, Loss: 0.5611, Train: 0.9143, Val: 0.7880, Test: 0.8020\n",
      "Epoch: 159, Loss: 0.5667, Train: 0.9143, Val: 0.7880, Test: 0.8020\n",
      "Epoch: 160, Loss: 0.5539, Train: 0.9071, Val: 0.7840, Test: 0.8020\n",
      "Epoch: 161, Loss: 0.6122, Train: 0.9071, Val: 0.7840, Test: 0.8020\n",
      "Epoch: 162, Loss: 0.5665, Train: 0.9143, Val: 0.7820, Test: 0.8020\n",
      "Epoch: 163, Loss: 0.5545, Train: 0.9071, Val: 0.7840, Test: 0.8020\n",
      "Epoch: 164, Loss: 0.5589, Train: 0.9143, Val: 0.7920, Test: 0.8020\n",
      "Epoch: 165, Loss: 0.5578, Train: 0.9143, Val: 0.7920, Test: 0.8020\n",
      "Epoch: 166, Loss: 0.6144, Train: 0.9214, Val: 0.7920, Test: 0.8020\n",
      "Epoch: 167, Loss: 0.5797, Train: 0.9286, Val: 0.7960, Test: 0.8020\n",
      "Epoch: 168, Loss: 0.5792, Train: 0.9286, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 169, Loss: 0.5738, Train: 0.9143, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 170, Loss: 0.5406, Train: 0.9071, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 171, Loss: 0.5769, Train: 0.9143, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 172, Loss: 0.5225, Train: 0.9214, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 173, Loss: 0.5717, Train: 0.9286, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 174, Loss: 0.5325, Train: 0.9286, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 175, Loss: 0.5603, Train: 0.9214, Val: 0.8000, Test: 0.8090\n",
      "Epoch: 176, Loss: 0.5789, Train: 0.9071, Val: 0.8020, Test: 0.8080\n",
      "Epoch: 177, Loss: 0.5100, Train: 0.9071, Val: 0.7960, Test: 0.8080\n",
      "Epoch: 178, Loss: 0.5272, Train: 0.9143, Val: 0.7900, Test: 0.8080\n",
      "Epoch: 179, Loss: 0.5826, Train: 0.9143, Val: 0.7840, Test: 0.8080\n",
      "Epoch: 180, Loss: 0.5379, Train: 0.9071, Val: 0.7860, Test: 0.8080\n",
      "Epoch: 181, Loss: 0.5389, Train: 0.9143, Val: 0.7840, Test: 0.8080\n",
      "Epoch: 182, Loss: 0.5340, Train: 0.9071, Val: 0.7800, Test: 0.8080\n",
      "Epoch: 183, Loss: 0.5261, Train: 0.9071, Val: 0.7860, Test: 0.8080\n",
      "Epoch: 184, Loss: 0.5410, Train: 0.9143, Val: 0.7980, Test: 0.8080\n",
      "Epoch: 185, Loss: 0.5439, Train: 0.9214, Val: 0.7960, Test: 0.8080\n",
      "Epoch: 186, Loss: 0.5549, Train: 0.9286, Val: 0.7980, Test: 0.8080\n",
      "Epoch: 187, Loss: 0.5041, Train: 0.9214, Val: 0.7960, Test: 0.8080\n",
      "Epoch: 188, Loss: 0.5072, Train: 0.9143, Val: 0.7960, Test: 0.8080\n",
      "Epoch: 189, Loss: 0.5242, Train: 0.9214, Val: 0.7960, Test: 0.8080\n",
      "Epoch: 190, Loss: 0.5057, Train: 0.9214, Val: 0.7980, Test: 0.8080\n",
      "Epoch: 191, Loss: 0.5099, Train: 0.9143, Val: 0.7980, Test: 0.8080\n",
      "Epoch: 192, Loss: 0.4877, Train: 0.9143, Val: 0.7960, Test: 0.8080\n",
      "Epoch: 193, Loss: 0.4801, Train: 0.9071, Val: 0.7900, Test: 0.8080\n",
      "Epoch: 194, Loss: 0.4930, Train: 0.9071, Val: 0.7880, Test: 0.8080\n",
      "Epoch: 195, Loss: 0.5132, Train: 0.9071, Val: 0.7860, Test: 0.8080\n",
      "Epoch: 196, Loss: 0.5113, Train: 0.9143, Val: 0.7880, Test: 0.8080\n",
      "Epoch: 197, Loss: 0.4984, Train: 0.9143, Val: 0.7840, Test: 0.8080\n",
      "Epoch: 198, Loss: 0.5089, Train: 0.9143, Val: 0.7860, Test: 0.8080\n",
      "Epoch: 199, Loss: 0.4950, Train: 0.9143, Val: 0.7920, Test: 0.8080\n",
      "Epoch: 200, Loss: 0.5109, Train: 0.9143, Val: 0.7980, Test: 0.8080\n"
     ]
    }
   ],
   "source": [
    "# RW version(symmetric하지 않지만, 다른 symmetric한 normalization을 찾을 수 없었음)\n",
    "!python gcn.py --problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5d809",
   "metadata": {},
   "source": [
    "### 2. normalization의 유무에 따른 embedding norm분포 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c2dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(path, \"Cora\", transform=T.NormalizeFeatures())\n",
    "z = model.encode(dataset.data.x,dataset.data.edge_index) #encode\n",
    "\n",
    "emb = TSNE(n_components=2, learning_rate='auto').fit_transform(z.detach().numpy())\n",
    "labels = dataset.data.y.detach().numpy()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "number_of_colors = len(np.unique(labels))\n",
    "\n",
    "color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "             for i in range(number_of_colors)]\n",
    "for idx , i in enumerate(np.unique(labels)) :\n",
    "    emb_ = emb[np.where(labels == i ),:].squeeze()\n",
    "    ax.scatter(x=emb_[:,0],y=emb_[:,1],c=color[idx], label=i,alpha=0.2)\n",
    "else :\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c766be80",
   "metadata": {},
   "source": [
    "### 3. normalization이 없을 때의 성능확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66efb86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Epoch: 001, Loss: 1.9448, Train: 0.3214, Val: 0.2420, Test: 0.2420\n",
      "Epoch: 002, Loss: 1.9401, Train: 0.2857, Val: 0.1800, Test: 0.2420\n",
      "Epoch: 003, Loss: 1.9339, Train: 0.3929, Val: 0.3040, Test: 0.2780\n",
      "Epoch: 004, Loss: 1.9272, Train: 0.4643, Val: 0.3660, Test: 0.3460\n",
      "Epoch: 005, Loss: 1.9185, Train: 0.4714, Val: 0.3800, Test: 0.3580\n",
      "Epoch: 006, Loss: 1.9110, Train: 0.5500, Val: 0.4220, Test: 0.3930\n",
      "Epoch: 007, Loss: 1.9037, Train: 0.5857, Val: 0.4340, Test: 0.4180\n",
      "Epoch: 008, Loss: 1.8928, Train: 0.6643, Val: 0.4800, Test: 0.4740\n",
      "Epoch: 009, Loss: 1.8840, Train: 0.6571, Val: 0.4840, Test: 0.4900\n",
      "Epoch: 010, Loss: 1.8699, Train: 0.6643, Val: 0.4780, Test: 0.4900\n",
      "Epoch: 011, Loss: 1.8621, Train: 0.6714, Val: 0.4500, Test: 0.4900\n",
      "Epoch: 012, Loss: 1.8496, Train: 0.6643, Val: 0.4620, Test: 0.4900\n",
      "Epoch: 013, Loss: 1.8476, Train: 0.6714, Val: 0.4580, Test: 0.4900\n",
      "Epoch: 014, Loss: 1.8354, Train: 0.6786, Val: 0.4820, Test: 0.4900\n",
      "Epoch: 015, Loss: 1.8188, Train: 0.7429, Val: 0.5540, Test: 0.5530\n",
      "Epoch: 016, Loss: 1.8059, Train: 0.8143, Val: 0.6180, Test: 0.6240\n",
      "Epoch: 017, Loss: 1.7976, Train: 0.8357, Val: 0.6600, Test: 0.6530\n",
      "Epoch: 018, Loss: 1.7739, Train: 0.8357, Val: 0.6860, Test: 0.6710\n",
      "Epoch: 019, Loss: 1.7645, Train: 0.8357, Val: 0.6760, Test: 0.6710\n",
      "Epoch: 020, Loss: 1.7539, Train: 0.8429, Val: 0.6940, Test: 0.6600\n",
      "Epoch: 021, Loss: 1.7472, Train: 0.8429, Val: 0.6960, Test: 0.6680\n",
      "Epoch: 022, Loss: 1.7350, Train: 0.8500, Val: 0.6940, Test: 0.6680\n",
      "Epoch: 023, Loss: 1.7103, Train: 0.8643, Val: 0.7080, Test: 0.6950\n",
      "Epoch: 024, Loss: 1.6845, Train: 0.8714, Val: 0.7220, Test: 0.7020\n",
      "Epoch: 025, Loss: 1.6808, Train: 0.8786, Val: 0.7300, Test: 0.7140\n",
      "Epoch: 026, Loss: 1.6672, Train: 0.8714, Val: 0.7400, Test: 0.7290\n",
      "Epoch: 027, Loss: 1.6600, Train: 0.8786, Val: 0.7460, Test: 0.7380\n",
      "Epoch: 028, Loss: 1.6372, Train: 0.8786, Val: 0.7500, Test: 0.7520\n",
      "Epoch: 029, Loss: 1.6238, Train: 0.8786, Val: 0.7540, Test: 0.7580\n",
      "Epoch: 030, Loss: 1.5957, Train: 0.8786, Val: 0.7580, Test: 0.7590\n",
      "Epoch: 031, Loss: 1.5832, Train: 0.8786, Val: 0.7600, Test: 0.7590\n",
      "Epoch: 032, Loss: 1.5830, Train: 0.8786, Val: 0.7520, Test: 0.7590\n",
      "Epoch: 033, Loss: 1.5311, Train: 0.8714, Val: 0.7480, Test: 0.7590\n",
      "Epoch: 034, Loss: 1.5280, Train: 0.8714, Val: 0.7380, Test: 0.7590\n",
      "Epoch: 035, Loss: 1.5120, Train: 0.8786, Val: 0.7480, Test: 0.7590\n",
      "Epoch: 036, Loss: 1.4811, Train: 0.8786, Val: 0.7500, Test: 0.7590\n",
      "Epoch: 037, Loss: 1.4598, Train: 0.8857, Val: 0.7500, Test: 0.7590\n",
      "Epoch: 038, Loss: 1.4356, Train: 0.8857, Val: 0.7540, Test: 0.7590\n",
      "Epoch: 039, Loss: 1.4391, Train: 0.8786, Val: 0.7560, Test: 0.7590\n",
      "Epoch: 040, Loss: 1.4324, Train: 0.8786, Val: 0.7640, Test: 0.7560\n",
      "Epoch: 041, Loss: 1.4102, Train: 0.8786, Val: 0.7680, Test: 0.7640\n",
      "Epoch: 042, Loss: 1.3991, Train: 0.8786, Val: 0.7680, Test: 0.7640\n",
      "Epoch: 043, Loss: 1.3344, Train: 0.8857, Val: 0.7720, Test: 0.7650\n",
      "Epoch: 044, Loss: 1.3416, Train: 0.8857, Val: 0.7740, Test: 0.7680\n",
      "Epoch: 045, Loss: 1.3193, Train: 0.8786, Val: 0.7740, Test: 0.7680\n",
      "Epoch: 046, Loss: 1.2985, Train: 0.8714, Val: 0.7740, Test: 0.7680\n",
      "Epoch: 047, Loss: 1.2770, Train: 0.8714, Val: 0.7680, Test: 0.7680\n",
      "Epoch: 048, Loss: 1.2561, Train: 0.8786, Val: 0.7660, Test: 0.7680\n",
      "Epoch: 049, Loss: 1.2778, Train: 0.8929, Val: 0.7660, Test: 0.7680\n",
      "Epoch: 050, Loss: 1.2317, Train: 0.8929, Val: 0.7680, Test: 0.7680\n",
      "Epoch: 051, Loss: 1.2468, Train: 0.8857, Val: 0.7700, Test: 0.7680\n",
      "Epoch: 052, Loss: 1.1839, Train: 0.8857, Val: 0.7820, Test: 0.7710\n",
      "Epoch: 053, Loss: 1.1756, Train: 0.8929, Val: 0.7820, Test: 0.7710\n",
      "Epoch: 054, Loss: 1.1358, Train: 0.9000, Val: 0.7860, Test: 0.7800\n",
      "Epoch: 055, Loss: 1.1380, Train: 0.9071, Val: 0.7900, Test: 0.7840\n",
      "Epoch: 056, Loss: 1.1011, Train: 0.9000, Val: 0.7920, Test: 0.7890\n",
      "Epoch: 057, Loss: 1.1238, Train: 0.9143, Val: 0.7920, Test: 0.7890\n",
      "Epoch: 058, Loss: 1.0827, Train: 0.9143, Val: 0.7940, Test: 0.7920\n",
      "Epoch: 059, Loss: 1.0880, Train: 0.9143, Val: 0.7980, Test: 0.7930\n",
      "Epoch: 060, Loss: 1.0628, Train: 0.9214, Val: 0.7960, Test: 0.7930\n",
      "Epoch: 061, Loss: 1.0479, Train: 0.9214, Val: 0.7960, Test: 0.7930\n",
      "Epoch: 062, Loss: 1.0227, Train: 0.9214, Val: 0.7940, Test: 0.7930\n",
      "Epoch: 063, Loss: 0.9890, Train: 0.9214, Val: 0.7920, Test: 0.7930\n",
      "Epoch: 064, Loss: 0.9597, Train: 0.9214, Val: 0.7960, Test: 0.7930\n",
      "Epoch: 065, Loss: 0.9284, Train: 0.9071, Val: 0.7940, Test: 0.7930\n",
      "Epoch: 066, Loss: 0.9712, Train: 0.9071, Val: 0.7920, Test: 0.7930\n",
      "Epoch: 067, Loss: 0.9118, Train: 0.9214, Val: 0.7900, Test: 0.7930\n",
      "Epoch: 068, Loss: 0.9201, Train: 0.9214, Val: 0.7940, Test: 0.7930\n",
      "Epoch: 069, Loss: 0.8961, Train: 0.9214, Val: 0.8000, Test: 0.7970\n",
      "Epoch: 070, Loss: 0.9119, Train: 0.9143, Val: 0.8040, Test: 0.7980\n",
      "Epoch: 071, Loss: 0.8634, Train: 0.9214, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 072, Loss: 0.8544, Train: 0.9214, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 073, Loss: 0.8815, Train: 0.9286, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 074, Loss: 0.8202, Train: 0.9286, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 075, Loss: 0.8785, Train: 0.9214, Val: 0.7960, Test: 0.7980\n",
      "Epoch: 076, Loss: 0.8335, Train: 0.9214, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 077, Loss: 0.8310, Train: 0.9286, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 078, Loss: 0.7990, Train: 0.9286, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 079, Loss: 0.7802, Train: 0.9214, Val: 0.7940, Test: 0.7980\n",
      "Epoch: 080, Loss: 0.7832, Train: 0.9214, Val: 0.7960, Test: 0.7980\n",
      "Epoch: 081, Loss: 0.7704, Train: 0.9214, Val: 0.7920, Test: 0.7980\n",
      "Epoch: 082, Loss: 0.7257, Train: 0.9286, Val: 0.7900, Test: 0.7980\n",
      "Epoch: 083, Loss: 0.8050, Train: 0.9286, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 084, Loss: 0.7445, Train: 0.9357, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 085, Loss: 0.7188, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 086, Loss: 0.6953, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 087, Loss: 0.7222, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 088, Loss: 0.7585, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 089, Loss: 0.6936, Train: 0.9357, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 090, Loss: 0.6962, Train: 0.9357, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 091, Loss: 0.7223, Train: 0.9357, Val: 0.7960, Test: 0.7980\n",
      "Epoch: 092, Loss: 0.6845, Train: 0.9357, Val: 0.7960, Test: 0.7980\n",
      "Epoch: 093, Loss: 0.6747, Train: 0.9357, Val: 0.7920, Test: 0.7980\n",
      "Epoch: 094, Loss: 0.6516, Train: 0.9357, Val: 0.7880, Test: 0.7980\n",
      "Epoch: 095, Loss: 0.6670, Train: 0.9286, Val: 0.7880, Test: 0.7980\n",
      "Epoch: 096, Loss: 0.6538, Train: 0.9286, Val: 0.7840, Test: 0.7980\n",
      "Epoch: 097, Loss: 0.6266, Train: 0.9286, Val: 0.7900, Test: 0.7980\n",
      "Epoch: 098, Loss: 0.6129, Train: 0.9214, Val: 0.7940, Test: 0.7980\n",
      "Epoch: 099, Loss: 0.6355, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 100, Loss: 0.6459, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 101, Loss: 0.5975, Train: 0.9286, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 102, Loss: 0.6280, Train: 0.9357, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 103, Loss: 0.6198, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 104, Loss: 0.6262, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 105, Loss: 0.5971, Train: 0.9286, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 106, Loss: 0.5831, Train: 0.9286, Val: 0.7980, Test: 0.7980\n",
      "Epoch: 107, Loss: 0.5975, Train: 0.9214, Val: 0.7920, Test: 0.7980\n",
      "Epoch: 108, Loss: 0.5826, Train: 0.9286, Val: 0.7960, Test: 0.7980\n",
      "Epoch: 109, Loss: 0.5851, Train: 0.9286, Val: 0.7920, Test: 0.7980\n",
      "Epoch: 110, Loss: 0.6117, Train: 0.9286, Val: 0.7940, Test: 0.7980\n",
      "Epoch: 111, Loss: 0.5750, Train: 0.9286, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 112, Loss: 0.5719, Train: 0.9286, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 113, Loss: 0.5760, Train: 0.9357, Val: 0.8020, Test: 0.7980\n",
      "Epoch: 114, Loss: 0.5783, Train: 0.9357, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 115, Loss: 0.5357, Train: 0.9429, Val: 0.8020, Test: 0.7980\n",
      "Epoch: 116, Loss: 0.5710, Train: 0.9429, Val: 0.8020, Test: 0.7980\n",
      "Epoch: 117, Loss: 0.5917, Train: 0.9429, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 118, Loss: 0.5295, Train: 0.9429, Val: 0.8020, Test: 0.7980\n",
      "Epoch: 119, Loss: 0.5354, Train: 0.9429, Val: 0.8020, Test: 0.7980\n",
      "Epoch: 120, Loss: 0.5454, Train: 0.9429, Val: 0.8040, Test: 0.7980\n",
      "Epoch: 121, Loss: 0.5590, Train: 0.9357, Val: 0.8080, Test: 0.8160\n",
      "Epoch: 122, Loss: 0.5490, Train: 0.9429, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 123, Loss: 0.5326, Train: 0.9429, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 124, Loss: 0.5141, Train: 0.9357, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 125, Loss: 0.5369, Train: 0.9357, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 126, Loss: 0.5191, Train: 0.9500, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 127, Loss: 0.5098, Train: 0.9500, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 128, Loss: 0.5415, Train: 0.9500, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 129, Loss: 0.5678, Train: 0.9429, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 130, Loss: 0.4888, Train: 0.9357, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 131, Loss: 0.5010, Train: 0.9429, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 132, Loss: 0.5276, Train: 0.9429, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 133, Loss: 0.5224, Train: 0.9500, Val: 0.8080, Test: 0.8160\n",
      "Epoch: 134, Loss: 0.5084, Train: 0.9429, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 135, Loss: 0.5157, Train: 0.9500, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 136, Loss: 0.5065, Train: 0.9571, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 137, Loss: 0.5092, Train: 0.9571, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 138, Loss: 0.5309, Train: 0.9571, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 139, Loss: 0.4815, Train: 0.9571, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 140, Loss: 0.4550, Train: 0.9429, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 141, Loss: 0.4929, Train: 0.9429, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 142, Loss: 0.4806, Train: 0.9500, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 143, Loss: 0.4687, Train: 0.9500, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 144, Loss: 0.4852, Train: 0.9500, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 145, Loss: 0.5223, Train: 0.9357, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 146, Loss: 0.4691, Train: 0.9357, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 147, Loss: 0.4417, Train: 0.9500, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 148, Loss: 0.4817, Train: 0.9500, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 149, Loss: 0.4648, Train: 0.9500, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 150, Loss: 0.4487, Train: 0.9429, Val: 0.8140, Test: 0.8150\n",
      "Epoch: 151, Loss: 0.4633, Train: 0.9429, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 152, Loss: 0.4551, Train: 0.9429, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 153, Loss: 0.4880, Train: 0.9429, Val: 0.8120, Test: 0.8150\n",
      "Epoch: 154, Loss: 0.4906, Train: 0.9429, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 155, Loss: 0.4243, Train: 0.9429, Val: 0.8120, Test: 0.8150\n",
      "Epoch: 156, Loss: 0.4476, Train: 0.9500, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 157, Loss: 0.4397, Train: 0.9571, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 158, Loss: 0.4351, Train: 0.9500, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 159, Loss: 0.4498, Train: 0.9500, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 160, Loss: 0.4428, Train: 0.9500, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 161, Loss: 0.4701, Train: 0.9571, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 162, Loss: 0.4576, Train: 0.9571, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 163, Loss: 0.4559, Train: 0.9571, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 164, Loss: 0.4075, Train: 0.9429, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 165, Loss: 0.4636, Train: 0.9429, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 166, Loss: 0.4474, Train: 0.9429, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 167, Loss: 0.4513, Train: 0.9571, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 168, Loss: 0.4699, Train: 0.9571, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 169, Loss: 0.4327, Train: 0.9643, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 170, Loss: 0.3922, Train: 0.9500, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 171, Loss: 0.4275, Train: 0.9500, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 172, Loss: 0.4583, Train: 0.9500, Val: 0.8120, Test: 0.8150\n",
      "Epoch: 173, Loss: 0.4414, Train: 0.9571, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 174, Loss: 0.4618, Train: 0.9571, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 175, Loss: 0.4302, Train: 0.9571, Val: 0.8120, Test: 0.8150\n",
      "Epoch: 176, Loss: 0.4525, Train: 0.9571, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 177, Loss: 0.4180, Train: 0.9571, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 178, Loss: 0.4479, Train: 0.9429, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 179, Loss: 0.4128, Train: 0.9571, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 180, Loss: 0.4075, Train: 0.9571, Val: 0.8140, Test: 0.8150\n",
      "Epoch: 181, Loss: 0.4128, Train: 0.9571, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 182, Loss: 0.4404, Train: 0.9571, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 183, Loss: 0.4347, Train: 0.9643, Val: 0.8120, Test: 0.8150\n",
      "Epoch: 184, Loss: 0.4327, Train: 0.9571, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 185, Loss: 0.4181, Train: 0.9643, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 186, Loss: 0.3968, Train: 0.9571, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 187, Loss: 0.4190, Train: 0.9571, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 188, Loss: 0.4291, Train: 0.9500, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 189, Loss: 0.4393, Train: 0.9500, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 190, Loss: 0.3795, Train: 0.9500, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 191, Loss: 0.4213, Train: 0.9571, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 192, Loss: 0.3590, Train: 0.9571, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 193, Loss: 0.4335, Train: 0.9571, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 194, Loss: 0.4138, Train: 0.9571, Val: 0.8120, Test: 0.8150\n",
      "Epoch: 195, Loss: 0.4218, Train: 0.9714, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 196, Loss: 0.4058, Train: 0.9714, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 197, Loss: 0.4480, Train: 0.9643, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 198, Loss: 0.3809, Train: 0.9714, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 199, Loss: 0.4310, Train: 0.9571, Val: 0.8100, Test: 0.8150\n",
      "Epoch: 200, Loss: 0.3837, Train: 0.9500, Val: 0.8080, Test: 0.8150\n"
     ]
    }
   ],
   "source": [
    "!python gcn.py --problem 3 # 이상하게 성능이 원래랑 같"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57f4e5",
   "metadata": {},
   "source": [
    "### 4. Cora 데이터의 각 label별로 training data, val, test data의 수를 학인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d48fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: 140\n",
      "val data: 500\n",
      "test data: 1000\n"
     ]
    }
   ],
   "source": [
    "!python gcn.py --problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11beaf1e",
   "metadata": {},
   "source": [
    "### 5. training data의 edge list가 undirected인지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4f21e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is undirected: True\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import is_undirected\n",
    "is_symmetric = is_undirected(data.edge_index)\n",
    "print(f'is undirected: {is_symmetric}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19073612",
   "metadata": {},
   "source": [
    "### 6. hyperparameter를 변경해 가며, 최고의 성능을 찾기 - 경쟁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5754a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d886ec8c",
   "metadata": {},
   "source": [
    "### 7. layer 하나를 더 쌓아 성능을 올리기 - 경쟁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3753b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "982e5af1",
   "metadata": {},
   "source": [
    "### C. PubMed에서 최고의 성적을 내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad29a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
