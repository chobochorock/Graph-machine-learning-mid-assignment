{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a29ae1b0",
   "metadata": {},
   "source": [
    "### 1. 두 가지의 symmetric normalization을 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6910e0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9463, Train: 0.1500, Val: 0.0600, Test: 0.0740\n",
      "Epoch: 002, Loss: 1.9429, Train: 0.3714, Val: 0.1820, Test: 0.2170\n",
      "Epoch: 003, Loss: 1.9402, Train: 0.2929, Val: 0.2360, Test: 0.2350\n",
      "Epoch: 004, Loss: 1.9342, Train: 0.4000, Val: 0.3220, Test: 0.3270\n",
      "Epoch: 005, Loss: 1.9319, Train: 0.5929, Val: 0.5680, Test: 0.5940\n",
      "Epoch: 006, Loss: 1.9257, Train: 0.6571, Val: 0.6360, Test: 0.6450\n",
      "Epoch: 007, Loss: 1.9200, Train: 0.7429, Val: 0.7160, Test: 0.7150\n",
      "Epoch: 008, Loss: 1.9184, Train: 0.7857, Val: 0.7200, Test: 0.7320\n",
      "Epoch: 009, Loss: 1.9131, Train: 0.7714, Val: 0.5800, Test: 0.7320\n",
      "Epoch: 010, Loss: 1.9045, Train: 0.7857, Val: 0.5680, Test: 0.7320\n",
      "Epoch: 011, Loss: 1.8963, Train: 0.8071, Val: 0.5960, Test: 0.7320\n",
      "Epoch: 012, Loss: 1.8913, Train: 0.8500, Val: 0.6320, Test: 0.7320\n",
      "Epoch: 013, Loss: 1.8838, Train: 0.8571, Val: 0.7460, Test: 0.7430\n",
      "Epoch: 014, Loss: 1.8704, Train: 0.8286, Val: 0.7620, Test: 0.7610\n",
      "Epoch: 015, Loss: 1.8651, Train: 0.8000, Val: 0.7460, Test: 0.7610\n",
      "Epoch: 016, Loss: 1.8502, Train: 0.8071, Val: 0.7380, Test: 0.7610\n",
      "Epoch: 017, Loss: 1.8452, Train: 0.8214, Val: 0.7440, Test: 0.7610\n",
      "Epoch: 018, Loss: 1.8421, Train: 0.8286, Val: 0.7600, Test: 0.7610\n",
      "Epoch: 019, Loss: 1.8166, Train: 0.8357, Val: 0.7540, Test: 0.7610\n",
      "Epoch: 020, Loss: 1.8120, Train: 0.8429, Val: 0.7540, Test: 0.7610\n",
      "Epoch: 021, Loss: 1.8096, Train: 0.8571, Val: 0.7580, Test: 0.7610\n",
      "Epoch: 022, Loss: 1.7987, Train: 0.8429, Val: 0.7720, Test: 0.7960\n",
      "Epoch: 023, Loss: 1.7819, Train: 0.8500, Val: 0.7800, Test: 0.7930\n",
      "Epoch: 024, Loss: 1.7642, Train: 0.8429, Val: 0.7920, Test: 0.7870\n",
      "Epoch: 025, Loss: 1.7455, Train: 0.8357, Val: 0.7780, Test: 0.7870\n",
      "Epoch: 026, Loss: 1.7210, Train: 0.8286, Val: 0.7740, Test: 0.7870\n",
      "Epoch: 027, Loss: 1.7218, Train: 0.8286, Val: 0.7840, Test: 0.7870\n",
      "Epoch: 028, Loss: 1.7026, Train: 0.8286, Val: 0.7800, Test: 0.7870\n",
      "Epoch: 029, Loss: 1.6761, Train: 0.8357, Val: 0.7740, Test: 0.7870\n",
      "Epoch: 030, Loss: 1.6781, Train: 0.8571, Val: 0.7820, Test: 0.7870\n",
      "Epoch: 031, Loss: 1.6672, Train: 0.8643, Val: 0.7820, Test: 0.7870\n",
      "Epoch: 032, Loss: 1.6499, Train: 0.8643, Val: 0.7760, Test: 0.7870\n",
      "Epoch: 033, Loss: 1.6394, Train: 0.8643, Val: 0.7760, Test: 0.7870\n",
      "Epoch: 034, Loss: 1.6094, Train: 0.8643, Val: 0.7740, Test: 0.7870\n",
      "Epoch: 035, Loss: 1.5887, Train: 0.8571, Val: 0.7760, Test: 0.7870\n",
      "Epoch: 036, Loss: 1.5662, Train: 0.8643, Val: 0.7840, Test: 0.7870\n",
      "Epoch: 037, Loss: 1.5470, Train: 0.8571, Val: 0.7800, Test: 0.7870\n",
      "Epoch: 038, Loss: 1.5445, Train: 0.8571, Val: 0.7800, Test: 0.7870\n",
      "Epoch: 039, Loss: 1.5480, Train: 0.8571, Val: 0.7760, Test: 0.7870\n",
      "Epoch: 040, Loss: 1.5110, Train: 0.8643, Val: 0.7840, Test: 0.7870\n",
      "Epoch: 041, Loss: 1.5013, Train: 0.8714, Val: 0.7960, Test: 0.7970\n",
      "Epoch: 042, Loss: 1.4639, Train: 0.8643, Val: 0.7900, Test: 0.7970\n",
      "Epoch: 043, Loss: 1.4371, Train: 0.8571, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 044, Loss: 1.4443, Train: 0.8571, Val: 0.7840, Test: 0.7970\n",
      "Epoch: 045, Loss: 1.4016, Train: 0.8643, Val: 0.7860, Test: 0.7970\n",
      "Epoch: 046, Loss: 1.3901, Train: 0.8714, Val: 0.7900, Test: 0.7970\n",
      "Epoch: 047, Loss: 1.3631, Train: 0.8714, Val: 0.7920, Test: 0.7970\n",
      "Epoch: 048, Loss: 1.3960, Train: 0.8714, Val: 0.7900, Test: 0.7970\n",
      "Epoch: 049, Loss: 1.3229, Train: 0.8714, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 050, Loss: 1.3438, Train: 0.8714, Val: 0.7900, Test: 0.7970\n",
      "Epoch: 051, Loss: 1.2872, Train: 0.8714, Val: 0.7920, Test: 0.7970\n",
      "Epoch: 052, Loss: 1.2810, Train: 0.8714, Val: 0.7960, Test: 0.7970\n",
      "Epoch: 053, Loss: 1.2749, Train: 0.8714, Val: 0.7940, Test: 0.7970\n",
      "Epoch: 054, Loss: 1.2813, Train: 0.8714, Val: 0.8000, Test: 0.8120\n",
      "Epoch: 055, Loss: 1.2541, Train: 0.8714, Val: 0.8080, Test: 0.8160\n",
      "Epoch: 056, Loss: 1.2181, Train: 0.8714, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 057, Loss: 1.2148, Train: 0.8714, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 058, Loss: 1.1887, Train: 0.8714, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 059, Loss: 1.1884, Train: 0.8857, Val: 0.7840, Test: 0.8160\n",
      "Epoch: 060, Loss: 1.1659, Train: 0.8714, Val: 0.7840, Test: 0.8160\n",
      "Epoch: 061, Loss: 1.1220, Train: 0.8786, Val: 0.7860, Test: 0.8160\n",
      "Epoch: 062, Loss: 1.1502, Train: 0.8857, Val: 0.7860, Test: 0.8160\n",
      "Epoch: 063, Loss: 1.1101, Train: 0.8857, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 064, Loss: 1.0674, Train: 0.8857, Val: 0.7880, Test: 0.8160\n",
      "Epoch: 065, Loss: 1.0838, Train: 0.8857, Val: 0.7860, Test: 0.8160\n",
      "Epoch: 066, Loss: 1.0949, Train: 0.8786, Val: 0.7860, Test: 0.8160\n",
      "Epoch: 067, Loss: 1.0675, Train: 0.8714, Val: 0.7880, Test: 0.8160\n",
      "Epoch: 068, Loss: 1.0075, Train: 0.8786, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 069, Loss: 1.0458, Train: 0.8786, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 070, Loss: 0.9949, Train: 0.8929, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 071, Loss: 0.9987, Train: 0.8929, Val: 0.8080, Test: 0.8160\n",
      "Epoch: 072, Loss: 0.9787, Train: 0.8857, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 073, Loss: 0.9770, Train: 0.8929, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 074, Loss: 0.9425, Train: 0.8929, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 075, Loss: 0.9860, Train: 0.9071, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 076, Loss: 0.9084, Train: 0.9143, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 077, Loss: 0.9444, Train: 0.9143, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 078, Loss: 0.9066, Train: 0.9143, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 079, Loss: 0.9003, Train: 0.9143, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 080, Loss: 0.8617, Train: 0.9000, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 081, Loss: 0.9193, Train: 0.9000, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 082, Loss: 0.8568, Train: 0.9071, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 083, Loss: 0.8599, Train: 0.9000, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 084, Loss: 0.8558, Train: 0.9000, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 085, Loss: 0.8568, Train: 0.9071, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 086, Loss: 0.8434, Train: 0.9214, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 087, Loss: 0.7964, Train: 0.9214, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 088, Loss: 0.7904, Train: 0.9286, Val: 0.7860, Test: 0.8160\n",
      "Epoch: 089, Loss: 0.8267, Train: 0.9214, Val: 0.7880, Test: 0.8160\n",
      "Epoch: 090, Loss: 0.8206, Train: 0.9214, Val: 0.7880, Test: 0.8160\n",
      "Epoch: 091, Loss: 0.7962, Train: 0.9143, Val: 0.7880, Test: 0.8160\n",
      "Epoch: 092, Loss: 0.7499, Train: 0.9143, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 093, Loss: 0.7571, Train: 0.9143, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 094, Loss: 0.8169, Train: 0.9143, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 095, Loss: 0.7253, Train: 0.9214, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 096, Loss: 0.7372, Train: 0.9286, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 097, Loss: 0.7687, Train: 0.9286, Val: 0.7880, Test: 0.8160\n",
      "Epoch: 098, Loss: 0.7723, Train: 0.9286, Val: 0.7840, Test: 0.8160\n",
      "Epoch: 099, Loss: 0.7563, Train: 0.9286, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 100, Loss: 0.7096, Train: 0.9214, Val: 0.7860, Test: 0.8160\n",
      "Epoch: 101, Loss: 0.7066, Train: 0.9214, Val: 0.7880, Test: 0.8160\n",
      "Epoch: 102, Loss: 0.6924, Train: 0.9214, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 103, Loss: 0.7112, Train: 0.9286, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 104, Loss: 0.6863, Train: 0.9286, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 105, Loss: 0.6697, Train: 0.9286, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 106, Loss: 0.6986, Train: 0.9286, Val: 0.7880, Test: 0.8160\n",
      "Epoch: 107, Loss: 0.6822, Train: 0.9214, Val: 0.7880, Test: 0.8160\n",
      "Epoch: 108, Loss: 0.6855, Train: 0.9214, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 109, Loss: 0.6442, Train: 0.9214, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 110, Loss: 0.7022, Train: 0.9286, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 111, Loss: 0.6805, Train: 0.9286, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 112, Loss: 0.6710, Train: 0.9286, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 113, Loss: 0.6351, Train: 0.9286, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 114, Loss: 0.6568, Train: 0.9286, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 115, Loss: 0.6651, Train: 0.9286, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 116, Loss: 0.6874, Train: 0.9286, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 117, Loss: 0.5840, Train: 0.9286, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 118, Loss: 0.6014, Train: 0.9286, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 119, Loss: 0.6128, Train: 0.9357, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 120, Loss: 0.5918, Train: 0.9286, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 121, Loss: 0.6322, Train: 0.9357, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 122, Loss: 0.6206, Train: 0.9357, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 123, Loss: 0.5810, Train: 0.9286, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 124, Loss: 0.5699, Train: 0.9286, Val: 0.7880, Test: 0.8160\n",
      "Epoch: 125, Loss: 0.5926, Train: 0.9286, Val: 0.7840, Test: 0.8160\n",
      "Epoch: 126, Loss: 0.6182, Train: 0.9214, Val: 0.7860, Test: 0.8160\n",
      "Epoch: 127, Loss: 0.5922, Train: 0.9286, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 128, Loss: 0.5988, Train: 0.9286, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 129, Loss: 0.5885, Train: 0.9214, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 130, Loss: 0.5637, Train: 0.9357, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 131, Loss: 0.6059, Train: 0.9357, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 132, Loss: 0.5576, Train: 0.9357, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 133, Loss: 0.5586, Train: 0.9357, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 134, Loss: 0.5772, Train: 0.9357, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 135, Loss: 0.5703, Train: 0.9357, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 136, Loss: 0.5454, Train: 0.9357, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 137, Loss: 0.5497, Train: 0.9357, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 138, Loss: 0.5560, Train: 0.9357, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 139, Loss: 0.5736, Train: 0.9357, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 140, Loss: 0.4888, Train: 0.9357, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 141, Loss: 0.5662, Train: 0.9357, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 142, Loss: 0.5546, Train: 0.9214, Val: 0.7800, Test: 0.8160\n",
      "Epoch: 143, Loss: 0.5222, Train: 0.9214, Val: 0.7780, Test: 0.8160\n",
      "Epoch: 144, Loss: 0.5162, Train: 0.9214, Val: 0.7800, Test: 0.8160\n",
      "Epoch: 145, Loss: 0.5468, Train: 0.9286, Val: 0.7880, Test: 0.8160\n",
      "Epoch: 146, Loss: 0.5527, Train: 0.9286, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 147, Loss: 0.5585, Train: 0.9357, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 148, Loss: 0.5576, Train: 0.9357, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 149, Loss: 0.5244, Train: 0.9357, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 150, Loss: 0.5413, Train: 0.9357, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 151, Loss: 0.5423, Train: 0.9286, Val: 0.8080, Test: 0.8160\n",
      "Epoch: 152, Loss: 0.5278, Train: 0.9286, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 153, Loss: 0.5779, Train: 0.9286, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 154, Loss: 0.5385, Train: 0.9286, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 155, Loss: 0.5256, Train: 0.9286, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 156, Loss: 0.5328, Train: 0.9286, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 157, Loss: 0.5329, Train: 0.9214, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 158, Loss: 0.5276, Train: 0.9214, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 159, Loss: 0.5386, Train: 0.9214, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 160, Loss: 0.4858, Train: 0.9286, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 161, Loss: 0.5090, Train: 0.9429, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 162, Loss: 0.4982, Train: 0.9429, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 163, Loss: 0.4832, Train: 0.9357, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 164, Loss: 0.5049, Train: 0.9429, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 165, Loss: 0.4948, Train: 0.9429, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 166, Loss: 0.4662, Train: 0.9429, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 167, Loss: 0.4808, Train: 0.9429, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 168, Loss: 0.4466, Train: 0.9429, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 169, Loss: 0.4718, Train: 0.9500, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 170, Loss: 0.4766, Train: 0.9500, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 171, Loss: 0.4801, Train: 0.9357, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 172, Loss: 0.4976, Train: 0.9357, Val: 0.7880, Test: 0.8160\n",
      "Epoch: 173, Loss: 0.5241, Train: 0.9429, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 174, Loss: 0.4609, Train: 0.9429, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 175, Loss: 0.4701, Train: 0.9429, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 176, Loss: 0.4406, Train: 0.9500, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 177, Loss: 0.4919, Train: 0.9429, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 178, Loss: 0.4726, Train: 0.9500, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 179, Loss: 0.4753, Train: 0.9500, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 180, Loss: 0.4964, Train: 0.9500, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 181, Loss: 0.4487, Train: 0.9429, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 182, Loss: 0.4831, Train: 0.9429, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 183, Loss: 0.4640, Train: 0.9429, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 184, Loss: 0.4512, Train: 0.9429, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 185, Loss: 0.4645, Train: 0.9500, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 186, Loss: 0.4549, Train: 0.9429, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 187, Loss: 0.4771, Train: 0.9429, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 188, Loss: 0.4270, Train: 0.9429, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 189, Loss: 0.4456, Train: 0.9429, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 190, Loss: 0.4578, Train: 0.9500, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 191, Loss: 0.4694, Train: 0.9429, Val: 0.8080, Test: 0.8160\n",
      "Epoch: 192, Loss: 0.4506, Train: 0.9500, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 193, Loss: 0.4560, Train: 0.9500, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 194, Loss: 0.4395, Train: 0.9500, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 195, Loss: 0.4211, Train: 0.9429, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 196, Loss: 0.4517, Train: 0.9429, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 197, Loss: 0.4509, Train: 0.9429, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 198, Loss: 0.4691, Train: 0.9429, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 199, Loss: 0.4367, Train: 0.9429, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 200, Loss: 0.4088, Train: 0.9429, Val: 0.8000, Test: 0.8160\n",
      "Median time per epoch: 0.0785s\n"
     ]
    }
   ],
   "source": [
    "!python gcn.py --use_gdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ed69e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9460, Train: 0.1786, Val: 0.0960, Test: 0.1070\n",
      "Epoch: 002, Loss: 1.9411, Train: 0.2500, Val: 0.1220, Test: 0.1550\n",
      "Epoch: 003, Loss: 1.9362, Train: 0.4857, Val: 0.2460, Test: 0.2810\n",
      "Epoch: 004, Loss: 1.9278, Train: 0.5571, Val: 0.3220, Test: 0.3190\n",
      "Epoch: 005, Loss: 1.9210, Train: 0.5857, Val: 0.3680, Test: 0.3890\n",
      "Epoch: 006, Loss: 1.9126, Train: 0.6143, Val: 0.4560, Test: 0.4820\n",
      "Epoch: 007, Loss: 1.9063, Train: 0.6786, Val: 0.5320, Test: 0.5510\n",
      "Epoch: 008, Loss: 1.9021, Train: 0.7286, Val: 0.5720, Test: 0.5930\n",
      "Epoch: 009, Loss: 1.8898, Train: 0.7786, Val: 0.6020, Test: 0.6130\n",
      "Epoch: 010, Loss: 1.8831, Train: 0.8000, Val: 0.6100, Test: 0.5850\n",
      "Epoch: 011, Loss: 1.8663, Train: 0.8214, Val: 0.6260, Test: 0.5890\n",
      "Epoch: 012, Loss: 1.8494, Train: 0.8571, Val: 0.6500, Test: 0.6240\n",
      "Epoch: 013, Loss: 1.8503, Train: 0.8786, Val: 0.6660, Test: 0.6520\n",
      "Epoch: 014, Loss: 1.8401, Train: 0.8929, Val: 0.6720, Test: 0.6810\n",
      "Epoch: 015, Loss: 1.8275, Train: 0.8714, Val: 0.6840, Test: 0.6850\n",
      "Epoch: 016, Loss: 1.7993, Train: 0.8786, Val: 0.7100, Test: 0.6970\n",
      "Epoch: 017, Loss: 1.8016, Train: 0.8929, Val: 0.7140, Test: 0.7080\n",
      "Epoch: 018, Loss: 1.7689, Train: 0.8857, Val: 0.7180, Test: 0.7180\n",
      "Epoch: 019, Loss: 1.7750, Train: 0.9000, Val: 0.7240, Test: 0.7270\n",
      "Epoch: 020, Loss: 1.7500, Train: 0.9071, Val: 0.7260, Test: 0.7380\n",
      "Epoch: 021, Loss: 1.7434, Train: 0.9071, Val: 0.7280, Test: 0.7480\n",
      "Epoch: 022, Loss: 1.7099, Train: 0.9143, Val: 0.7340, Test: 0.7530\n",
      "Epoch: 023, Loss: 1.6983, Train: 0.9214, Val: 0.7360, Test: 0.7480\n",
      "Epoch: 024, Loss: 1.6847, Train: 0.9214, Val: 0.7340, Test: 0.7480\n",
      "Epoch: 025, Loss: 1.6849, Train: 0.9286, Val: 0.7320, Test: 0.7480\n",
      "Epoch: 026, Loss: 1.6552, Train: 0.9429, Val: 0.7420, Test: 0.7540\n",
      "Epoch: 027, Loss: 1.6226, Train: 0.9429, Val: 0.7500, Test: 0.7620\n",
      "Epoch: 028, Loss: 1.6037, Train: 0.9643, Val: 0.7640, Test: 0.7670\n",
      "Epoch: 029, Loss: 1.6208, Train: 0.9786, Val: 0.7720, Test: 0.7740\n",
      "Epoch: 030, Loss: 1.6057, Train: 0.9786, Val: 0.7700, Test: 0.7740\n",
      "Epoch: 031, Loss: 1.5722, Train: 0.9714, Val: 0.7600, Test: 0.7740\n",
      "Epoch: 032, Loss: 1.5738, Train: 0.9714, Val: 0.7520, Test: 0.7740\n",
      "Epoch: 033, Loss: 1.5275, Train: 0.9714, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 034, Loss: 1.5070, Train: 0.9714, Val: 0.7500, Test: 0.7740\n",
      "Epoch: 035, Loss: 1.5161, Train: 0.9714, Val: 0.7440, Test: 0.7740\n",
      "Epoch: 036, Loss: 1.4798, Train: 0.9786, Val: 0.7500, Test: 0.7740\n",
      "Epoch: 037, Loss: 1.4537, Train: 0.9786, Val: 0.7540, Test: 0.7740\n",
      "Epoch: 038, Loss: 1.4626, Train: 0.9714, Val: 0.7520, Test: 0.7740\n",
      "Epoch: 039, Loss: 1.4284, Train: 0.9714, Val: 0.7540, Test: 0.7740\n",
      "Epoch: 040, Loss: 1.4210, Train: 0.9786, Val: 0.7560, Test: 0.7740\n",
      "Epoch: 041, Loss: 1.3686, Train: 0.9786, Val: 0.7620, Test: 0.7740\n",
      "Epoch: 042, Loss: 1.3594, Train: 0.9786, Val: 0.7740, Test: 0.7770\n",
      "Epoch: 043, Loss: 1.3473, Train: 0.9857, Val: 0.7760, Test: 0.7750\n",
      "Epoch: 044, Loss: 1.3021, Train: 0.9857, Val: 0.7760, Test: 0.7750\n",
      "Epoch: 045, Loss: 1.3264, Train: 0.9857, Val: 0.7780, Test: 0.7750\n",
      "Epoch: 046, Loss: 1.2762, Train: 0.9929, Val: 0.7820, Test: 0.7790\n",
      "Epoch: 047, Loss: 1.2584, Train: 0.9929, Val: 0.7760, Test: 0.7790\n",
      "Epoch: 048, Loss: 1.1920, Train: 0.9929, Val: 0.7780, Test: 0.7790\n",
      "Epoch: 049, Loss: 1.2099, Train: 0.9929, Val: 0.7700, Test: 0.7790\n",
      "Epoch: 050, Loss: 1.1733, Train: 0.9929, Val: 0.7700, Test: 0.7790\n",
      "Epoch: 051, Loss: 1.1770, Train: 0.9857, Val: 0.7680, Test: 0.7790\n",
      "Epoch: 052, Loss: 1.1708, Train: 0.9929, Val: 0.7660, Test: 0.7790\n",
      "Epoch: 053, Loss: 1.1248, Train: 0.9929, Val: 0.7680, Test: 0.7790\n",
      "Epoch: 054, Loss: 1.1109, Train: 0.9929, Val: 0.7720, Test: 0.7790\n",
      "Epoch: 055, Loss: 1.1183, Train: 0.9929, Val: 0.7720, Test: 0.7790\n",
      "Epoch: 056, Loss: 1.0849, Train: 0.9857, Val: 0.7720, Test: 0.7790\n",
      "Epoch: 057, Loss: 1.0898, Train: 0.9857, Val: 0.7720, Test: 0.7790\n",
      "Epoch: 058, Loss: 1.0418, Train: 0.9857, Val: 0.7680, Test: 0.7790\n",
      "Epoch: 059, Loss: 1.0466, Train: 0.9857, Val: 0.7680, Test: 0.7790\n",
      "Epoch: 060, Loss: 1.0337, Train: 0.9857, Val: 0.7660, Test: 0.7790\n",
      "Epoch: 061, Loss: 0.9676, Train: 0.9857, Val: 0.7660, Test: 0.7790\n",
      "Epoch: 062, Loss: 0.9993, Train: 0.9929, Val: 0.7640, Test: 0.7790\n",
      "Epoch: 063, Loss: 1.0074, Train: 0.9857, Val: 0.7680, Test: 0.7790\n",
      "Epoch: 064, Loss: 0.9677, Train: 0.9857, Val: 0.7680, Test: 0.7790\n",
      "Epoch: 065, Loss: 0.9285, Train: 0.9786, Val: 0.7680, Test: 0.7790\n",
      "Epoch: 066, Loss: 0.9621, Train: 0.9786, Val: 0.7700, Test: 0.7790\n",
      "Epoch: 067, Loss: 0.9277, Train: 0.9857, Val: 0.7740, Test: 0.7790\n",
      "Epoch: 068, Loss: 0.8840, Train: 0.9857, Val: 0.7760, Test: 0.7790\n",
      "Epoch: 069, Loss: 0.8751, Train: 0.9929, Val: 0.7760, Test: 0.7790\n",
      "Epoch: 070, Loss: 0.8839, Train: 0.9929, Val: 0.7820, Test: 0.7790\n",
      "Epoch: 071, Loss: 0.8637, Train: 0.9929, Val: 0.7820, Test: 0.7790\n",
      "Epoch: 072, Loss: 0.8663, Train: 0.9929, Val: 0.7800, Test: 0.7790\n",
      "Epoch: 073, Loss: 0.8336, Train: 0.9929, Val: 0.7860, Test: 0.8020\n",
      "Epoch: 074, Loss: 0.7904, Train: 0.9929, Val: 0.7880, Test: 0.8080\n",
      "Epoch: 075, Loss: 0.8113, Train: 0.9929, Val: 0.7880, Test: 0.8080\n",
      "Epoch: 076, Loss: 0.7726, Train: 0.9929, Val: 0.7860, Test: 0.8080\n",
      "Epoch: 077, Loss: 0.7827, Train: 0.9929, Val: 0.7880, Test: 0.8080\n",
      "Epoch: 078, Loss: 0.7983, Train: 0.9857, Val: 0.7880, Test: 0.8080\n",
      "Epoch: 079, Loss: 0.7811, Train: 0.9857, Val: 0.7920, Test: 0.8030\n",
      "Epoch: 080, Loss: 0.7604, Train: 0.9857, Val: 0.7880, Test: 0.8030\n",
      "Epoch: 081, Loss: 0.7332, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 082, Loss: 0.7502, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 083, Loss: 0.7641, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 084, Loss: 0.7275, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 085, Loss: 0.6607, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 086, Loss: 0.7122, Train: 0.9929, Val: 0.7760, Test: 0.8030\n",
      "Epoch: 087, Loss: 0.7126, Train: 0.9929, Val: 0.7740, Test: 0.8030\n",
      "Epoch: 088, Loss: 0.6639, Train: 0.9929, Val: 0.7760, Test: 0.8030\n",
      "Epoch: 089, Loss: 0.6649, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 090, Loss: 0.6847, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 091, Loss: 0.7074, Train: 0.9929, Val: 0.7780, Test: 0.8030\n",
      "Epoch: 092, Loss: 0.6853, Train: 0.9929, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 093, Loss: 0.6011, Train: 0.9929, Val: 0.7880, Test: 0.8030\n",
      "Epoch: 094, Loss: 0.6344, Train: 0.9929, Val: 0.7880, Test: 0.8030\n",
      "Epoch: 095, Loss: 0.6478, Train: 0.9929, Val: 0.7860, Test: 0.8030\n",
      "Epoch: 096, Loss: 0.6265, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 097, Loss: 0.6633, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 098, Loss: 0.6342, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 099, Loss: 0.6443, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 100, Loss: 0.6256, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 101, Loss: 0.5727, Train: 0.9929, Val: 0.7860, Test: 0.8030\n",
      "Epoch: 102, Loss: 0.6097, Train: 0.9929, Val: 0.7860, Test: 0.8030\n",
      "Epoch: 103, Loss: 0.5660, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 104, Loss: 0.5679, Train: 0.9929, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 105, Loss: 0.6116, Train: 0.9929, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 106, Loss: 0.5563, Train: 0.9929, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 107, Loss: 0.6315, Train: 0.9929, Val: 0.7860, Test: 0.8030\n",
      "Epoch: 108, Loss: 0.5389, Train: 0.9929, Val: 0.7860, Test: 0.8030\n",
      "Epoch: 109, Loss: 0.5319, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 110, Loss: 0.5585, Train: 0.9929, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 111, Loss: 0.5315, Train: 0.9929, Val: 0.7780, Test: 0.8030\n",
      "Epoch: 112, Loss: 0.5272, Train: 0.9929, Val: 0.7760, Test: 0.8030\n",
      "Epoch: 113, Loss: 0.5494, Train: 0.9929, Val: 0.7780, Test: 0.8030\n",
      "Epoch: 114, Loss: 0.5677, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 115, Loss: 0.5566, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 116, Loss: 0.5467, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 117, Loss: 0.5304, Train: 0.9929, Val: 0.7860, Test: 0.8030\n",
      "Epoch: 118, Loss: 0.4793, Train: 0.9929, Val: 0.7860, Test: 0.8030\n",
      "Epoch: 119, Loss: 0.4997, Train: 0.9929, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 120, Loss: 0.5635, Train: 0.9929, Val: 0.7780, Test: 0.8030\n",
      "Epoch: 121, Loss: 0.4902, Train: 0.9857, Val: 0.7780, Test: 0.8030\n",
      "Epoch: 122, Loss: 0.4999, Train: 0.9857, Val: 0.7780, Test: 0.8030\n",
      "Epoch: 123, Loss: 0.5674, Train: 0.9857, Val: 0.7760, Test: 0.8030\n",
      "Epoch: 124, Loss: 0.4845, Train: 0.9857, Val: 0.7720, Test: 0.8030\n",
      "Epoch: 125, Loss: 0.5022, Train: 0.9857, Val: 0.7760, Test: 0.8030\n",
      "Epoch: 126, Loss: 0.4938, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 127, Loss: 0.4954, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 128, Loss: 0.5010, Train: 0.9929, Val: 0.7860, Test: 0.8030\n",
      "Epoch: 129, Loss: 0.4795, Train: 0.9929, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 130, Loss: 0.5113, Train: 0.9929, Val: 0.7780, Test: 0.8030\n",
      "Epoch: 131, Loss: 0.4943, Train: 0.9929, Val: 0.7780, Test: 0.8030\n",
      "Epoch: 132, Loss: 0.4704, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 133, Loss: 0.5010, Train: 0.9929, Val: 0.7780, Test: 0.8030\n",
      "Epoch: 134, Loss: 0.4856, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 135, Loss: 0.5215, Train: 0.9929, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 136, Loss: 0.5047, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 137, Loss: 0.4773, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 138, Loss: 0.4400, Train: 0.9929, Val: 0.7780, Test: 0.8030\n",
      "Epoch: 139, Loss: 0.5154, Train: 0.9857, Val: 0.7760, Test: 0.8030\n",
      "Epoch: 140, Loss: 0.4531, Train: 0.9857, Val: 0.7760, Test: 0.8030\n",
      "Epoch: 141, Loss: 0.4710, Train: 0.9929, Val: 0.7740, Test: 0.8030\n",
      "Epoch: 142, Loss: 0.4398, Train: 0.9929, Val: 0.7780, Test: 0.8030\n",
      "Epoch: 143, Loss: 0.4510, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 144, Loss: 0.4488, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 145, Loss: 0.4277, Train: 0.9929, Val: 0.7760, Test: 0.8030\n",
      "Epoch: 146, Loss: 0.4444, Train: 0.9929, Val: 0.7780, Test: 0.8030\n",
      "Epoch: 147, Loss: 0.4194, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 148, Loss: 0.4654, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 149, Loss: 0.3940, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 150, Loss: 0.4302, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 151, Loss: 0.5112, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 152, Loss: 0.4029, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 153, Loss: 0.4372, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 154, Loss: 0.4352, Train: 0.9929, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 155, Loss: 0.4114, Train: 0.9929, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 156, Loss: 0.4196, Train: 0.9929, Val: 0.7860, Test: 0.8030\n",
      "Epoch: 157, Loss: 0.3952, Train: 0.9929, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 158, Loss: 0.4199, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 159, Loss: 0.3920, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 160, Loss: 0.3763, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 161, Loss: 0.3858, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 162, Loss: 0.3861, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 163, Loss: 0.4418, Train: 0.9929, Val: 0.7780, Test: 0.8030\n",
      "Epoch: 164, Loss: 0.3727, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 165, Loss: 0.4049, Train: 0.9929, Val: 0.7740, Test: 0.8030\n",
      "Epoch: 166, Loss: 0.4061, Train: 0.9929, Val: 0.7740, Test: 0.8030\n",
      "Epoch: 167, Loss: 0.4087, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 168, Loss: 0.4454, Train: 0.9929, Val: 0.7780, Test: 0.8030\n",
      "Epoch: 169, Loss: 0.3677, Train: 0.9929, Val: 0.7860, Test: 0.8030\n",
      "Epoch: 170, Loss: 0.4544, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 171, Loss: 0.3815, Train: 1.0000, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 172, Loss: 0.3712, Train: 1.0000, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 173, Loss: 0.4073, Train: 1.0000, Val: 0.7760, Test: 0.8030\n",
      "Epoch: 174, Loss: 0.3451, Train: 1.0000, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 175, Loss: 0.4365, Train: 1.0000, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 176, Loss: 0.3615, Train: 0.9929, Val: 0.7760, Test: 0.8030\n",
      "Epoch: 177, Loss: 0.3938, Train: 0.9929, Val: 0.7780, Test: 0.8030\n",
      "Epoch: 178, Loss: 0.3797, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 179, Loss: 0.3627, Train: 0.9929, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 180, Loss: 0.4201, Train: 0.9929, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 181, Loss: 0.3475, Train: 0.9929, Val: 0.7800, Test: 0.8030\n",
      "Epoch: 182, Loss: 0.3999, Train: 0.9929, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 183, Loss: 0.3953, Train: 0.9929, Val: 0.7860, Test: 0.8030\n",
      "Epoch: 184, Loss: 0.4325, Train: 0.9929, Val: 0.7900, Test: 0.8030\n",
      "Epoch: 185, Loss: 0.4046, Train: 0.9929, Val: 0.7880, Test: 0.8030\n",
      "Epoch: 186, Loss: 0.3603, Train: 1.0000, Val: 0.7860, Test: 0.8030\n",
      "Epoch: 187, Loss: 0.3888, Train: 1.0000, Val: 0.7900, Test: 0.8030\n",
      "Epoch: 188, Loss: 0.3692, Train: 1.0000, Val: 0.7840, Test: 0.8030\n",
      "Epoch: 189, Loss: 0.3716, Train: 1.0000, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 190, Loss: 0.3455, Train: 1.0000, Val: 0.7760, Test: 0.8030\n",
      "Epoch: 191, Loss: 0.3629, Train: 1.0000, Val: 0.7760, Test: 0.8030\n",
      "Epoch: 192, Loss: 0.3887, Train: 1.0000, Val: 0.7760, Test: 0.8030\n",
      "Epoch: 193, Loss: 0.3588, Train: 1.0000, Val: 0.7720, Test: 0.8030\n",
      "Epoch: 194, Loss: 0.3845, Train: 0.9929, Val: 0.7740, Test: 0.8030\n",
      "Epoch: 195, Loss: 0.3937, Train: 0.9929, Val: 0.7760, Test: 0.8030\n",
      "Epoch: 196, Loss: 0.4042, Train: 0.9929, Val: 0.7860, Test: 0.8030\n",
      "Epoch: 197, Loss: 0.3690, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 198, Loss: 0.3540, Train: 0.9929, Val: 0.7780, Test: 0.8030\n",
      "Epoch: 199, Loss: 0.3268, Train: 0.9929, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 200, Loss: 0.3577, Train: 0.9929, Val: 0.7860, Test: 0.8030\n",
      "Median time per epoch: 0.0445s\n"
     ]
    }
   ],
   "source": [
    "!python gcn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5d809",
   "metadata": {},
   "source": [
    "### 2. normalization의 유무에 따른 embedding norm분포 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c2dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c766be80",
   "metadata": {},
   "source": [
    "### 3. normalization이 없을 때의 성능확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66efb86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb57f4e5",
   "metadata": {},
   "source": [
    "### 4. Cora 데이터의 각 label별로 training data, val, test data의 수를 학인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d48fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11beaf1e",
   "metadata": {},
   "source": [
    "### 5. training data의 edge list가 undirected인지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f21e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19073612",
   "metadata": {},
   "source": [
    "### 6. hyperparameter를 변경해 가며, 최고의 성능을 찾기 - 경쟁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5754a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d886ec8c",
   "metadata": {},
   "source": [
    "### 7. layer 하나를 더 쌓아 성능을 올리기 - 경쟁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3753b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "982e5af1",
   "metadata": {},
   "source": [
    "### C. PubMed에서 최고의 성적을 내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad29a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
