{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a29ae1b0",
   "metadata": {},
   "source": [
    "### 1. 두 가지의 symmetric normalization을 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6910e0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use normalization: True\n",
      "Epoch: 001, Loss: 1.9462, Train: 0.3571, Val: 0.2700, Test: 0.2530\n",
      "Epoch: 002, Loss: 1.9428, Train: 0.5143, Val: 0.4460, Test: 0.4120\n",
      "Epoch: 003, Loss: 1.9390, Train: 0.5714, Val: 0.6060, Test: 0.6040\n",
      "Epoch: 004, Loss: 1.9367, Train: 0.6214, Val: 0.6300, Test: 0.6280\n",
      "Epoch: 005, Loss: 1.9328, Train: 0.7429, Val: 0.6520, Test: 0.6770\n",
      "Epoch: 006, Loss: 1.9255, Train: 0.7643, Val: 0.6540, Test: 0.6580\n",
      "Epoch: 007, Loss: 1.9217, Train: 0.8071, Val: 0.6680, Test: 0.6520\n",
      "Epoch: 008, Loss: 1.9168, Train: 0.8143, Val: 0.6860, Test: 0.6850\n",
      "Epoch: 009, Loss: 1.9099, Train: 0.8286, Val: 0.7200, Test: 0.7310\n",
      "Epoch: 010, Loss: 1.8997, Train: 0.8286, Val: 0.7380, Test: 0.7440\n",
      "Epoch: 011, Loss: 1.8951, Train: 0.8429, Val: 0.7280, Test: 0.7440\n",
      "Epoch: 012, Loss: 1.8859, Train: 0.8429, Val: 0.7280, Test: 0.7440\n",
      "Epoch: 013, Loss: 1.8787, Train: 0.8643, Val: 0.7340, Test: 0.7440\n",
      "Epoch: 014, Loss: 1.8684, Train: 0.8714, Val: 0.7360, Test: 0.7440\n",
      "Epoch: 015, Loss: 1.8606, Train: 0.9000, Val: 0.7360, Test: 0.7440\n",
      "Epoch: 016, Loss: 1.8513, Train: 0.8857, Val: 0.7200, Test: 0.7440\n",
      "Epoch: 017, Loss: 1.8423, Train: 0.8786, Val: 0.7080, Test: 0.7440\n",
      "Epoch: 018, Loss: 1.8351, Train: 0.8786, Val: 0.7140, Test: 0.7440\n",
      "Epoch: 019, Loss: 1.8179, Train: 0.8857, Val: 0.7500, Test: 0.7320\n",
      "Epoch: 020, Loss: 1.8122, Train: 0.9000, Val: 0.7740, Test: 0.7710\n",
      "Epoch: 021, Loss: 1.7995, Train: 0.8929, Val: 0.7640, Test: 0.7710\n",
      "Epoch: 022, Loss: 1.7902, Train: 0.8786, Val: 0.7760, Test: 0.7840\n",
      "Epoch: 023, Loss: 1.7725, Train: 0.8857, Val: 0.7820, Test: 0.7820\n",
      "Epoch: 024, Loss: 1.7636, Train: 0.8786, Val: 0.7800, Test: 0.7820\n",
      "Epoch: 025, Loss: 1.7580, Train: 0.9000, Val: 0.7700, Test: 0.7820\n",
      "Epoch: 026, Loss: 1.7269, Train: 0.8929, Val: 0.7720, Test: 0.7820\n",
      "Epoch: 027, Loss: 1.7209, Train: 0.8857, Val: 0.7700, Test: 0.7820\n",
      "Epoch: 028, Loss: 1.7239, Train: 0.8714, Val: 0.7480, Test: 0.7820\n",
      "Epoch: 029, Loss: 1.7064, Train: 0.8714, Val: 0.7380, Test: 0.7820\n",
      "Epoch: 030, Loss: 1.6998, Train: 0.8500, Val: 0.7300, Test: 0.7820\n",
      "Epoch: 031, Loss: 1.6732, Train: 0.8500, Val: 0.7320, Test: 0.7820\n",
      "Epoch: 032, Loss: 1.6377, Train: 0.8571, Val: 0.7360, Test: 0.7820\n",
      "Epoch: 033, Loss: 1.6617, Train: 0.8571, Val: 0.7360, Test: 0.7820\n",
      "Epoch: 034, Loss: 1.6320, Train: 0.8643, Val: 0.7340, Test: 0.7820\n",
      "Epoch: 035, Loss: 1.6208, Train: 0.8714, Val: 0.7280, Test: 0.7820\n",
      "Epoch: 036, Loss: 1.5945, Train: 0.8786, Val: 0.7360, Test: 0.7820\n",
      "Epoch: 037, Loss: 1.5734, Train: 0.8714, Val: 0.7480, Test: 0.7820\n",
      "Epoch: 038, Loss: 1.5669, Train: 0.8643, Val: 0.7520, Test: 0.7820\n",
      "Epoch: 039, Loss: 1.5371, Train: 0.8643, Val: 0.7640, Test: 0.7820\n",
      "Epoch: 040, Loss: 1.5337, Train: 0.8714, Val: 0.7780, Test: 0.7820\n",
      "Epoch: 041, Loss: 1.5206, Train: 0.8714, Val: 0.7800, Test: 0.7820\n",
      "Epoch: 042, Loss: 1.4847, Train: 0.8643, Val: 0.7800, Test: 0.7820\n",
      "Epoch: 043, Loss: 1.4695, Train: 0.8500, Val: 0.7860, Test: 0.7790\n",
      "Epoch: 044, Loss: 1.4637, Train: 0.8643, Val: 0.7880, Test: 0.7840\n",
      "Epoch: 045, Loss: 1.4496, Train: 0.8500, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 046, Loss: 1.4301, Train: 0.8500, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 047, Loss: 1.3931, Train: 0.8500, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 048, Loss: 1.4002, Train: 0.8571, Val: 0.7740, Test: 0.7840\n",
      "Epoch: 049, Loss: 1.3681, Train: 0.8500, Val: 0.7760, Test: 0.7840\n",
      "Epoch: 050, Loss: 1.3441, Train: 0.8571, Val: 0.7760, Test: 0.7840\n",
      "Epoch: 051, Loss: 1.3466, Train: 0.8571, Val: 0.7680, Test: 0.7840\n",
      "Epoch: 052, Loss: 1.2954, Train: 0.8714, Val: 0.7780, Test: 0.7840\n",
      "Epoch: 053, Loss: 1.2812, Train: 0.8857, Val: 0.7860, Test: 0.7840\n",
      "Epoch: 054, Loss: 1.2780, Train: 0.8786, Val: 0.7860, Test: 0.7840\n",
      "Epoch: 055, Loss: 1.2529, Train: 0.8786, Val: 0.7960, Test: 0.8030\n",
      "Epoch: 056, Loss: 1.2253, Train: 0.8857, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 057, Loss: 1.2283, Train: 0.8857, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 058, Loss: 1.2070, Train: 0.8714, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 059, Loss: 1.1502, Train: 0.8500, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 060, Loss: 1.1882, Train: 0.8500, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 061, Loss: 1.1326, Train: 0.8571, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 062, Loss: 1.1322, Train: 0.8571, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 063, Loss: 1.0985, Train: 0.8500, Val: 0.7820, Test: 0.8060\n",
      "Epoch: 064, Loss: 1.0886, Train: 0.8571, Val: 0.7820, Test: 0.8060\n",
      "Epoch: 065, Loss: 1.1089, Train: 0.8571, Val: 0.7840, Test: 0.8060\n",
      "Epoch: 066, Loss: 1.0680, Train: 0.8571, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 067, Loss: 1.0379, Train: 0.8643, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 068, Loss: 1.0343, Train: 0.8857, Val: 0.7840, Test: 0.8060\n",
      "Epoch: 069, Loss: 1.0059, Train: 0.9000, Val: 0.7840, Test: 0.8060\n",
      "Epoch: 070, Loss: 1.0023, Train: 0.9000, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 071, Loss: 0.9941, Train: 0.9071, Val: 0.8000, Test: 0.8060\n",
      "Epoch: 072, Loss: 1.0081, Train: 0.9000, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 073, Loss: 0.9885, Train: 0.9071, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 074, Loss: 0.8987, Train: 0.9000, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 075, Loss: 0.9466, Train: 0.9000, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 076, Loss: 0.9001, Train: 0.8929, Val: 0.7800, Test: 0.8060\n",
      "Epoch: 077, Loss: 0.9050, Train: 0.8714, Val: 0.7780, Test: 0.8060\n",
      "Epoch: 078, Loss: 0.8952, Train: 0.8714, Val: 0.7740, Test: 0.8060\n",
      "Epoch: 079, Loss: 0.8699, Train: 0.8714, Val: 0.7760, Test: 0.8060\n",
      "Epoch: 080, Loss: 0.8793, Train: 0.8786, Val: 0.7800, Test: 0.8060\n",
      "Epoch: 081, Loss: 0.8592, Train: 0.8929, Val: 0.7840, Test: 0.8060\n",
      "Epoch: 082, Loss: 0.8638, Train: 0.9071, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 083, Loss: 0.8328, Train: 0.9071, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 084, Loss: 0.8535, Train: 0.9143, Val: 0.8000, Test: 0.8060\n",
      "Epoch: 085, Loss: 0.8108, Train: 0.9143, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 086, Loss: 0.7907, Train: 0.9071, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 087, Loss: 0.8465, Train: 0.9000, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 088, Loss: 0.7771, Train: 0.9000, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 089, Loss: 0.7941, Train: 0.9000, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 090, Loss: 0.7692, Train: 0.9000, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 091, Loss: 0.7248, Train: 0.9214, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 092, Loss: 0.7198, Train: 0.9214, Val: 0.8000, Test: 0.8060\n",
      "Epoch: 093, Loss: 0.7475, Train: 0.9143, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 094, Loss: 0.7583, Train: 0.9143, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 095, Loss: 0.7450, Train: 0.9143, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 096, Loss: 0.7293, Train: 0.9143, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 097, Loss: 0.7646, Train: 0.9143, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 098, Loss: 0.7324, Train: 0.9143, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 099, Loss: 0.7258, Train: 0.9143, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 100, Loss: 0.6812, Train: 0.9214, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 101, Loss: 0.6919, Train: 0.9214, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 102, Loss: 0.6909, Train: 0.9214, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 103, Loss: 0.7154, Train: 0.9214, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 104, Loss: 0.6548, Train: 0.9143, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 105, Loss: 0.6809, Train: 0.9143, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 106, Loss: 0.6615, Train: 0.9143, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 107, Loss: 0.6698, Train: 0.9143, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 108, Loss: 0.6623, Train: 0.9214, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 109, Loss: 0.6326, Train: 0.9214, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 110, Loss: 0.6620, Train: 0.9214, Val: 0.8000, Test: 0.8060\n",
      "Epoch: 111, Loss: 0.6232, Train: 0.9214, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 112, Loss: 0.6365, Train: 0.9214, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 113, Loss: 0.6096, Train: 0.9286, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 114, Loss: 0.6200, Train: 0.9214, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 115, Loss: 0.5828, Train: 0.9214, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 116, Loss: 0.5892, Train: 0.9286, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 117, Loss: 0.5962, Train: 0.9286, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 118, Loss: 0.6113, Train: 0.9143, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 119, Loss: 0.6075, Train: 0.9214, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 120, Loss: 0.5762, Train: 0.9214, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 121, Loss: 0.5924, Train: 0.9214, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 122, Loss: 0.5673, Train: 0.9286, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 123, Loss: 0.6043, Train: 0.9286, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 124, Loss: 0.5868, Train: 0.9357, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 125, Loss: 0.5714, Train: 0.9429, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 126, Loss: 0.5812, Train: 0.9429, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 127, Loss: 0.5707, Train: 0.9357, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 128, Loss: 0.5491, Train: 0.9286, Val: 0.7940, Test: 0.8150\n",
      "Epoch: 129, Loss: 0.5560, Train: 0.9214, Val: 0.7960, Test: 0.8150\n",
      "Epoch: 130, Loss: 0.5217, Train: 0.9214, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 131, Loss: 0.5795, Train: 0.9286, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 132, Loss: 0.5350, Train: 0.9214, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 133, Loss: 0.5743, Train: 0.9286, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 134, Loss: 0.5362, Train: 0.9286, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 135, Loss: 0.5349, Train: 0.9286, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 136, Loss: 0.5588, Train: 0.9429, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 137, Loss: 0.5442, Train: 0.9429, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 138, Loss: 0.5110, Train: 0.9357, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 139, Loss: 0.5318, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 140, Loss: 0.5086, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 141, Loss: 0.5035, Train: 0.9357, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 142, Loss: 0.4724, Train: 0.9357, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 143, Loss: 0.5268, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 144, Loss: 0.4975, Train: 0.9429, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 145, Loss: 0.5304, Train: 0.9357, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 146, Loss: 0.4907, Train: 0.9357, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 147, Loss: 0.4839, Train: 0.9357, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 148, Loss: 0.4917, Train: 0.9357, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 149, Loss: 0.4993, Train: 0.9357, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 150, Loss: 0.4918, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 151, Loss: 0.5034, Train: 0.9357, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 152, Loss: 0.4858, Train: 0.9429, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 153, Loss: 0.4308, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 154, Loss: 0.4585, Train: 0.9357, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 155, Loss: 0.4331, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 156, Loss: 0.4445, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 157, Loss: 0.4676, Train: 0.9429, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 158, Loss: 0.4631, Train: 0.9429, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 159, Loss: 0.4575, Train: 0.9357, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 160, Loss: 0.4628, Train: 0.9429, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 161, Loss: 0.4923, Train: 0.9429, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 162, Loss: 0.4599, Train: 0.9429, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 163, Loss: 0.4759, Train: 0.9357, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 164, Loss: 0.4427, Train: 0.9429, Val: 0.8120, Test: 0.8170\n",
      "Epoch: 165, Loss: 0.4531, Train: 0.9429, Val: 0.8120, Test: 0.8170\n",
      "Epoch: 166, Loss: 0.4823, Train: 0.9429, Val: 0.8100, Test: 0.8170\n",
      "Epoch: 167, Loss: 0.4451, Train: 0.9429, Val: 0.8120, Test: 0.8170\n",
      "Epoch: 168, Loss: 0.4870, Train: 0.9429, Val: 0.8100, Test: 0.8170\n",
      "Epoch: 169, Loss: 0.4404, Train: 0.9357, Val: 0.8100, Test: 0.8170\n",
      "Epoch: 170, Loss: 0.4547, Train: 0.9429, Val: 0.8060, Test: 0.8170\n",
      "Epoch: 171, Loss: 0.4506, Train: 0.9429, Val: 0.8040, Test: 0.8170\n",
      "Epoch: 172, Loss: 0.4293, Train: 0.9429, Val: 0.8040, Test: 0.8170\n",
      "Epoch: 173, Loss: 0.4475, Train: 0.9429, Val: 0.8040, Test: 0.8170\n",
      "Epoch: 174, Loss: 0.4129, Train: 0.9429, Val: 0.8000, Test: 0.8170\n",
      "Epoch: 175, Loss: 0.4485, Train: 0.9571, Val: 0.8040, Test: 0.8170\n",
      "Epoch: 176, Loss: 0.4683, Train: 0.9429, Val: 0.8140, Test: 0.8160\n",
      "Epoch: 177, Loss: 0.3939, Train: 0.9429, Val: 0.8100, Test: 0.8160\n",
      "Epoch: 178, Loss: 0.4068, Train: 0.9500, Val: 0.8100, Test: 0.8160\n",
      "Epoch: 179, Loss: 0.4619, Train: 0.9429, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 180, Loss: 0.4509, Train: 0.9429, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 181, Loss: 0.4464, Train: 0.9429, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 182, Loss: 0.4279, Train: 0.9429, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 183, Loss: 0.4161, Train: 0.9500, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 184, Loss: 0.4318, Train: 0.9571, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 185, Loss: 0.4289, Train: 0.9571, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 186, Loss: 0.4248, Train: 0.9571, Val: 0.8100, Test: 0.8160\n",
      "Epoch: 187, Loss: 0.4023, Train: 0.9643, Val: 0.8140, Test: 0.8160\n",
      "Epoch: 188, Loss: 0.3923, Train: 0.9643, Val: 0.8120, Test: 0.8160\n",
      "Epoch: 189, Loss: 0.3943, Train: 0.9643, Val: 0.8120, Test: 0.8160\n",
      "Epoch: 190, Loss: 0.3826, Train: 0.9500, Val: 0.8080, Test: 0.8160\n",
      "Epoch: 191, Loss: 0.4394, Train: 0.9571, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 192, Loss: 0.3902, Train: 0.9500, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 193, Loss: 0.3751, Train: 0.9500, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 194, Loss: 0.3863, Train: 0.9500, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 195, Loss: 0.4223, Train: 0.9500, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 196, Loss: 0.3728, Train: 0.9643, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 197, Loss: 0.3817, Train: 0.9643, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 198, Loss: 0.3699, Train: 0.9643, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 199, Loss: 0.3922, Train: 0.9643, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 200, Loss: 0.3875, Train: 0.9643, Val: 0.8080, Test: 0.8160\n"
     ]
    }
   ],
   "source": [
    "# symmetric version (default)\n",
    "!python gcn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed69e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use normalization: True\n",
      "Epoch: 001, Loss: 1.9462, Train: 0.3571, Val: 0.2700, Test: 0.2530\n",
      "Epoch: 002, Loss: 1.9428, Train: 0.5143, Val: 0.4460, Test: 0.4120\n",
      "Epoch: 003, Loss: 1.9390, Train: 0.5714, Val: 0.6060, Test: 0.6040\n",
      "Epoch: 004, Loss: 1.9367, Train: 0.6214, Val: 0.6300, Test: 0.6280\n",
      "Epoch: 005, Loss: 1.9328, Train: 0.7429, Val: 0.6520, Test: 0.6770\n",
      "Epoch: 006, Loss: 1.9255, Train: 0.7643, Val: 0.6540, Test: 0.6580\n",
      "Epoch: 007, Loss: 1.9217, Train: 0.8071, Val: 0.6680, Test: 0.6520\n",
      "Epoch: 008, Loss: 1.9168, Train: 0.8143, Val: 0.6860, Test: 0.6850\n",
      "Epoch: 009, Loss: 1.9099, Train: 0.8286, Val: 0.7200, Test: 0.7310\n",
      "Epoch: 010, Loss: 1.8997, Train: 0.8286, Val: 0.7380, Test: 0.7440\n",
      "Epoch: 011, Loss: 1.8951, Train: 0.8429, Val: 0.7280, Test: 0.7440\n",
      "Epoch: 012, Loss: 1.8859, Train: 0.8429, Val: 0.7280, Test: 0.7440\n",
      "Epoch: 013, Loss: 1.8787, Train: 0.8643, Val: 0.7340, Test: 0.7440\n",
      "Epoch: 014, Loss: 1.8684, Train: 0.8714, Val: 0.7360, Test: 0.7440\n",
      "Epoch: 015, Loss: 1.8606, Train: 0.9000, Val: 0.7360, Test: 0.7440\n",
      "Epoch: 016, Loss: 1.8513, Train: 0.8857, Val: 0.7200, Test: 0.7440\n",
      "Epoch: 017, Loss: 1.8423, Train: 0.8786, Val: 0.7080, Test: 0.7440\n",
      "Epoch: 018, Loss: 1.8351, Train: 0.8786, Val: 0.7140, Test: 0.7440\n",
      "Epoch: 019, Loss: 1.8179, Train: 0.8857, Val: 0.7500, Test: 0.7320\n",
      "Epoch: 020, Loss: 1.8122, Train: 0.9000, Val: 0.7740, Test: 0.7710\n",
      "Epoch: 021, Loss: 1.7995, Train: 0.8929, Val: 0.7640, Test: 0.7710\n",
      "Epoch: 022, Loss: 1.7902, Train: 0.8786, Val: 0.7760, Test: 0.7840\n",
      "Epoch: 023, Loss: 1.7725, Train: 0.8857, Val: 0.7820, Test: 0.7820\n",
      "Epoch: 024, Loss: 1.7636, Train: 0.8786, Val: 0.7800, Test: 0.7820\n",
      "Epoch: 025, Loss: 1.7580, Train: 0.9000, Val: 0.7700, Test: 0.7820\n",
      "Epoch: 026, Loss: 1.7269, Train: 0.8929, Val: 0.7720, Test: 0.7820\n",
      "Epoch: 027, Loss: 1.7209, Train: 0.8857, Val: 0.7700, Test: 0.7820\n",
      "Epoch: 028, Loss: 1.7239, Train: 0.8714, Val: 0.7480, Test: 0.7820\n",
      "Epoch: 029, Loss: 1.7064, Train: 0.8714, Val: 0.7380, Test: 0.7820\n",
      "Epoch: 030, Loss: 1.6998, Train: 0.8500, Val: 0.7300, Test: 0.7820\n",
      "Epoch: 031, Loss: 1.6732, Train: 0.8500, Val: 0.7320, Test: 0.7820\n",
      "Epoch: 032, Loss: 1.6377, Train: 0.8571, Val: 0.7360, Test: 0.7820\n",
      "Epoch: 033, Loss: 1.6617, Train: 0.8571, Val: 0.7360, Test: 0.7820\n",
      "Epoch: 034, Loss: 1.6320, Train: 0.8643, Val: 0.7340, Test: 0.7820\n",
      "Epoch: 035, Loss: 1.6208, Train: 0.8714, Val: 0.7280, Test: 0.7820\n",
      "Epoch: 036, Loss: 1.5945, Train: 0.8786, Val: 0.7360, Test: 0.7820\n",
      "Epoch: 037, Loss: 1.5734, Train: 0.8714, Val: 0.7480, Test: 0.7820\n",
      "Epoch: 038, Loss: 1.5669, Train: 0.8643, Val: 0.7520, Test: 0.7820\n",
      "Epoch: 039, Loss: 1.5371, Train: 0.8643, Val: 0.7640, Test: 0.7820\n",
      "Epoch: 040, Loss: 1.5337, Train: 0.8714, Val: 0.7780, Test: 0.7820\n",
      "Epoch: 041, Loss: 1.5206, Train: 0.8714, Val: 0.7800, Test: 0.7820\n",
      "Epoch: 042, Loss: 1.4847, Train: 0.8643, Val: 0.7800, Test: 0.7820\n",
      "Epoch: 043, Loss: 1.4695, Train: 0.8500, Val: 0.7860, Test: 0.7790\n",
      "Epoch: 044, Loss: 1.4637, Train: 0.8643, Val: 0.7880, Test: 0.7840\n",
      "Epoch: 045, Loss: 1.4496, Train: 0.8500, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 046, Loss: 1.4301, Train: 0.8500, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 047, Loss: 1.3931, Train: 0.8500, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 048, Loss: 1.4002, Train: 0.8571, Val: 0.7740, Test: 0.7840\n",
      "Epoch: 049, Loss: 1.3681, Train: 0.8500, Val: 0.7760, Test: 0.7840\n",
      "Epoch: 050, Loss: 1.3441, Train: 0.8571, Val: 0.7760, Test: 0.7840\n",
      "Epoch: 051, Loss: 1.3466, Train: 0.8571, Val: 0.7680, Test: 0.7840\n",
      "Epoch: 052, Loss: 1.2954, Train: 0.8714, Val: 0.7780, Test: 0.7840\n",
      "Epoch: 053, Loss: 1.2812, Train: 0.8857, Val: 0.7860, Test: 0.7840\n",
      "Epoch: 054, Loss: 1.2780, Train: 0.8786, Val: 0.7860, Test: 0.7840\n",
      "Epoch: 055, Loss: 1.2529, Train: 0.8786, Val: 0.7960, Test: 0.8030\n",
      "Epoch: 056, Loss: 1.2253, Train: 0.8857, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 057, Loss: 1.2283, Train: 0.8857, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 058, Loss: 1.2070, Train: 0.8714, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 059, Loss: 1.1502, Train: 0.8500, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 060, Loss: 1.1882, Train: 0.8500, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 061, Loss: 1.1326, Train: 0.8571, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 062, Loss: 1.1322, Train: 0.8571, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 063, Loss: 1.0985, Train: 0.8500, Val: 0.7820, Test: 0.8060\n",
      "Epoch: 064, Loss: 1.0886, Train: 0.8571, Val: 0.7820, Test: 0.8060\n",
      "Epoch: 065, Loss: 1.1089, Train: 0.8571, Val: 0.7840, Test: 0.8060\n",
      "Epoch: 066, Loss: 1.0680, Train: 0.8571, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 067, Loss: 1.0379, Train: 0.8643, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 068, Loss: 1.0343, Train: 0.8857, Val: 0.7840, Test: 0.8060\n",
      "Epoch: 069, Loss: 1.0059, Train: 0.9000, Val: 0.7840, Test: 0.8060\n",
      "Epoch: 070, Loss: 1.0023, Train: 0.9000, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 071, Loss: 0.9941, Train: 0.9071, Val: 0.8000, Test: 0.8060\n",
      "Epoch: 072, Loss: 1.0081, Train: 0.9000, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 073, Loss: 0.9885, Train: 0.9071, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 074, Loss: 0.8987, Train: 0.9000, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 075, Loss: 0.9466, Train: 0.9000, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 076, Loss: 0.9001, Train: 0.8929, Val: 0.7800, Test: 0.8060\n",
      "Epoch: 077, Loss: 0.9050, Train: 0.8714, Val: 0.7780, Test: 0.8060\n",
      "Epoch: 078, Loss: 0.8952, Train: 0.8714, Val: 0.7740, Test: 0.8060\n",
      "Epoch: 079, Loss: 0.8699, Train: 0.8714, Val: 0.7760, Test: 0.8060\n",
      "Epoch: 080, Loss: 0.8793, Train: 0.8786, Val: 0.7800, Test: 0.8060\n",
      "Epoch: 081, Loss: 0.8592, Train: 0.8929, Val: 0.7840, Test: 0.8060\n",
      "Epoch: 082, Loss: 0.8638, Train: 0.9071, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 083, Loss: 0.8328, Train: 0.9071, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 084, Loss: 0.8535, Train: 0.9143, Val: 0.8000, Test: 0.8060\n",
      "Epoch: 085, Loss: 0.8108, Train: 0.9143, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 086, Loss: 0.7907, Train: 0.9071, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 087, Loss: 0.8465, Train: 0.9000, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 088, Loss: 0.7771, Train: 0.9000, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 089, Loss: 0.7941, Train: 0.9000, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 090, Loss: 0.7692, Train: 0.9000, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 091, Loss: 0.7248, Train: 0.9214, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 092, Loss: 0.7198, Train: 0.9214, Val: 0.8000, Test: 0.8060\n",
      "Epoch: 093, Loss: 0.7475, Train: 0.9143, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 094, Loss: 0.7583, Train: 0.9143, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 095, Loss: 0.7450, Train: 0.9143, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 096, Loss: 0.7293, Train: 0.9143, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 097, Loss: 0.7646, Train: 0.9143, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 098, Loss: 0.7324, Train: 0.9143, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 099, Loss: 0.7258, Train: 0.9143, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 100, Loss: 0.6812, Train: 0.9214, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 101, Loss: 0.6919, Train: 0.9214, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 102, Loss: 0.6909, Train: 0.9214, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 103, Loss: 0.7154, Train: 0.9214, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 104, Loss: 0.6548, Train: 0.9143, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 105, Loss: 0.6809, Train: 0.9143, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 106, Loss: 0.6615, Train: 0.9143, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 107, Loss: 0.6698, Train: 0.9143, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 108, Loss: 0.6623, Train: 0.9214, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 109, Loss: 0.6326, Train: 0.9214, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 110, Loss: 0.6620, Train: 0.9214, Val: 0.8000, Test: 0.8060\n",
      "Epoch: 111, Loss: 0.6232, Train: 0.9214, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 112, Loss: 0.6365, Train: 0.9214, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 113, Loss: 0.6096, Train: 0.9286, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 114, Loss: 0.6200, Train: 0.9214, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 115, Loss: 0.5828, Train: 0.9214, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 116, Loss: 0.5892, Train: 0.9286, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 117, Loss: 0.5962, Train: 0.9286, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 118, Loss: 0.6113, Train: 0.9143, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 119, Loss: 0.6075, Train: 0.9214, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 120, Loss: 0.5762, Train: 0.9214, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 121, Loss: 0.5924, Train: 0.9214, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 122, Loss: 0.5673, Train: 0.9286, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 123, Loss: 0.6043, Train: 0.9286, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 124, Loss: 0.5868, Train: 0.9357, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 125, Loss: 0.5714, Train: 0.9429, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 126, Loss: 0.5812, Train: 0.9429, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 127, Loss: 0.5707, Train: 0.9357, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 128, Loss: 0.5491, Train: 0.9286, Val: 0.7940, Test: 0.8150\n",
      "Epoch: 129, Loss: 0.5560, Train: 0.9214, Val: 0.7960, Test: 0.8150\n",
      "Epoch: 130, Loss: 0.5217, Train: 0.9214, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 131, Loss: 0.5795, Train: 0.9286, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 132, Loss: 0.5350, Train: 0.9214, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 133, Loss: 0.5743, Train: 0.9286, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 134, Loss: 0.5362, Train: 0.9286, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 135, Loss: 0.5349, Train: 0.9286, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 136, Loss: 0.5588, Train: 0.9429, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 137, Loss: 0.5442, Train: 0.9429, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 138, Loss: 0.5110, Train: 0.9357, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 139, Loss: 0.5318, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 140, Loss: 0.5086, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 141, Loss: 0.5035, Train: 0.9357, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 142, Loss: 0.4724, Train: 0.9357, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 143, Loss: 0.5268, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 144, Loss: 0.4975, Train: 0.9429, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 145, Loss: 0.5304, Train: 0.9357, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 146, Loss: 0.4907, Train: 0.9357, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 147, Loss: 0.4839, Train: 0.9357, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 148, Loss: 0.4917, Train: 0.9357, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 149, Loss: 0.4993, Train: 0.9357, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 150, Loss: 0.4918, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 151, Loss: 0.5034, Train: 0.9357, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 152, Loss: 0.4858, Train: 0.9429, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 153, Loss: 0.4308, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 154, Loss: 0.4585, Train: 0.9357, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 155, Loss: 0.4331, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 156, Loss: 0.4445, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 157, Loss: 0.4676, Train: 0.9429, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 158, Loss: 0.4631, Train: 0.9429, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 159, Loss: 0.4575, Train: 0.9357, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 160, Loss: 0.4628, Train: 0.9429, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 161, Loss: 0.4923, Train: 0.9429, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 162, Loss: 0.4599, Train: 0.9429, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 163, Loss: 0.4759, Train: 0.9357, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 164, Loss: 0.4427, Train: 0.9429, Val: 0.8120, Test: 0.8170\n",
      "Epoch: 165, Loss: 0.4531, Train: 0.9429, Val: 0.8120, Test: 0.8170\n",
      "Epoch: 166, Loss: 0.4823, Train: 0.9429, Val: 0.8100, Test: 0.8170\n",
      "Epoch: 167, Loss: 0.4451, Train: 0.9429, Val: 0.8120, Test: 0.8170\n",
      "Epoch: 168, Loss: 0.4870, Train: 0.9429, Val: 0.8100, Test: 0.8170\n",
      "Epoch: 169, Loss: 0.4404, Train: 0.9357, Val: 0.8100, Test: 0.8170\n",
      "Epoch: 170, Loss: 0.4547, Train: 0.9429, Val: 0.8060, Test: 0.8170\n",
      "Epoch: 171, Loss: 0.4506, Train: 0.9429, Val: 0.8040, Test: 0.8170\n",
      "Epoch: 172, Loss: 0.4293, Train: 0.9429, Val: 0.8040, Test: 0.8170\n",
      "Epoch: 173, Loss: 0.4475, Train: 0.9429, Val: 0.8040, Test: 0.8170\n",
      "Epoch: 174, Loss: 0.4129, Train: 0.9429, Val: 0.8000, Test: 0.8170\n",
      "Epoch: 175, Loss: 0.4485, Train: 0.9571, Val: 0.8040, Test: 0.8170\n",
      "Epoch: 176, Loss: 0.4683, Train: 0.9429, Val: 0.8140, Test: 0.8160\n",
      "Epoch: 177, Loss: 0.3939, Train: 0.9429, Val: 0.8100, Test: 0.8160\n",
      "Epoch: 178, Loss: 0.4068, Train: 0.9500, Val: 0.8100, Test: 0.8160\n",
      "Epoch: 179, Loss: 0.4619, Train: 0.9429, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 180, Loss: 0.4509, Train: 0.9429, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 181, Loss: 0.4464, Train: 0.9429, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 182, Loss: 0.4279, Train: 0.9429, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 183, Loss: 0.4161, Train: 0.9500, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 184, Loss: 0.4318, Train: 0.9571, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 185, Loss: 0.4289, Train: 0.9571, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 186, Loss: 0.4248, Train: 0.9571, Val: 0.8100, Test: 0.8160\n",
      "Epoch: 187, Loss: 0.4023, Train: 0.9643, Val: 0.8140, Test: 0.8160\n",
      "Epoch: 188, Loss: 0.3923, Train: 0.9643, Val: 0.8120, Test: 0.8160\n",
      "Epoch: 189, Loss: 0.3943, Train: 0.9643, Val: 0.8120, Test: 0.8160\n",
      "Epoch: 190, Loss: 0.3826, Train: 0.9500, Val: 0.8080, Test: 0.8160\n",
      "Epoch: 191, Loss: 0.4394, Train: 0.9571, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 192, Loss: 0.3902, Train: 0.9500, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 193, Loss: 0.3751, Train: 0.9500, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 194, Loss: 0.3863, Train: 0.9500, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 195, Loss: 0.4223, Train: 0.9500, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 196, Loss: 0.3728, Train: 0.9643, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 197, Loss: 0.3817, Train: 0.9643, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 198, Loss: 0.3699, Train: 0.9643, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 199, Loss: 0.3922, Train: 0.9643, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 200, Loss: 0.3875, Train: 0.9643, Val: 0.8080, Test: 0.8160\n"
     ]
    }
   ],
   "source": [
    "# RW version(symmetric하지 않지만, 다른 symmetric한 normalization을 찾을 수 없었음, 또는 improved 옵션을 사용한 결과 원래의 결과와 같았음)\n",
    "!python gcn.py --problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5d809",
   "metadata": {},
   "source": [
    "### 2. normalization의 유무에 따른 embedding norm분포 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c2dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(path, \"Cora\", transform=T.NormalizeFeatures())\n",
    "z = model.encode(dataset.data.x,dataset.data.edge_index) #encode\n",
    "\n",
    "emb = TSNE(n_components=2, learning_rate='auto').fit_transform(z.detach().numpy())\n",
    "labels = dataset.data.y.detach().numpy()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "number_of_colors = len(np.unique(labels))\n",
    "\n",
    "color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "             for i in range(number_of_colors)]\n",
    "for idx , i in enumerate(np.unique(labels)) :\n",
    "    emb_ = emb[np.where(labels == i ),:].squeeze()\n",
    "    ax.scatter(x=emb_[:,0],y=emb_[:,1],c=color[idx], label=i,alpha=0.2)\n",
    "else :\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c766be80",
   "metadata": {},
   "source": [
    "### 3. normalization이 없을 때의 성능확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66efb86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use normalization: False\n",
      "Epoch: 001, Loss: 1.9462, Train: 0.3571, Val: 0.2700, Test: 0.2530\n",
      "Epoch: 002, Loss: 1.9428, Train: 0.5143, Val: 0.4460, Test: 0.4120\n",
      "Epoch: 003, Loss: 1.9390, Train: 0.5714, Val: 0.6060, Test: 0.6040\n",
      "Epoch: 004, Loss: 1.9367, Train: 0.6214, Val: 0.6300, Test: 0.6280\n",
      "Epoch: 005, Loss: 1.9328, Train: 0.7429, Val: 0.6520, Test: 0.6770\n",
      "Epoch: 006, Loss: 1.9255, Train: 0.7643, Val: 0.6540, Test: 0.6580\n",
      "Epoch: 007, Loss: 1.9217, Train: 0.8071, Val: 0.6680, Test: 0.6520\n",
      "Epoch: 008, Loss: 1.9168, Train: 0.8143, Val: 0.6860, Test: 0.6850\n",
      "Epoch: 009, Loss: 1.9099, Train: 0.8286, Val: 0.7200, Test: 0.7310\n",
      "Epoch: 010, Loss: 1.8997, Train: 0.8286, Val: 0.7380, Test: 0.7440\n",
      "Epoch: 011, Loss: 1.8951, Train: 0.8429, Val: 0.7280, Test: 0.7440\n",
      "Epoch: 012, Loss: 1.8859, Train: 0.8429, Val: 0.7280, Test: 0.7440\n",
      "Epoch: 013, Loss: 1.8787, Train: 0.8643, Val: 0.7340, Test: 0.7440\n",
      "Epoch: 014, Loss: 1.8684, Train: 0.8714, Val: 0.7360, Test: 0.7440\n",
      "Epoch: 015, Loss: 1.8606, Train: 0.9000, Val: 0.7360, Test: 0.7440\n",
      "Epoch: 016, Loss: 1.8513, Train: 0.8857, Val: 0.7200, Test: 0.7440\n",
      "Epoch: 017, Loss: 1.8423, Train: 0.8786, Val: 0.7080, Test: 0.7440\n",
      "Epoch: 018, Loss: 1.8351, Train: 0.8786, Val: 0.7140, Test: 0.7440\n",
      "Epoch: 019, Loss: 1.8179, Train: 0.8857, Val: 0.7500, Test: 0.7320\n",
      "Epoch: 020, Loss: 1.8122, Train: 0.9000, Val: 0.7740, Test: 0.7710\n",
      "Epoch: 021, Loss: 1.7995, Train: 0.8929, Val: 0.7640, Test: 0.7710\n",
      "Epoch: 022, Loss: 1.7902, Train: 0.8786, Val: 0.7760, Test: 0.7840\n",
      "Epoch: 023, Loss: 1.7725, Train: 0.8857, Val: 0.7820, Test: 0.7820\n",
      "Epoch: 024, Loss: 1.7636, Train: 0.8786, Val: 0.7800, Test: 0.7820\n",
      "Epoch: 025, Loss: 1.7580, Train: 0.9000, Val: 0.7700, Test: 0.7820\n",
      "Epoch: 026, Loss: 1.7269, Train: 0.8929, Val: 0.7720, Test: 0.7820\n",
      "Epoch: 027, Loss: 1.7209, Train: 0.8857, Val: 0.7700, Test: 0.7820\n",
      "Epoch: 028, Loss: 1.7239, Train: 0.8714, Val: 0.7480, Test: 0.7820\n",
      "Epoch: 029, Loss: 1.7064, Train: 0.8714, Val: 0.7380, Test: 0.7820\n",
      "Epoch: 030, Loss: 1.6998, Train: 0.8500, Val: 0.7300, Test: 0.7820\n",
      "Epoch: 031, Loss: 1.6732, Train: 0.8500, Val: 0.7320, Test: 0.7820\n",
      "Epoch: 032, Loss: 1.6377, Train: 0.8571, Val: 0.7360, Test: 0.7820\n",
      "Epoch: 033, Loss: 1.6617, Train: 0.8571, Val: 0.7360, Test: 0.7820\n",
      "Epoch: 034, Loss: 1.6320, Train: 0.8643, Val: 0.7340, Test: 0.7820\n",
      "Epoch: 035, Loss: 1.6208, Train: 0.8714, Val: 0.7280, Test: 0.7820\n",
      "Epoch: 036, Loss: 1.5945, Train: 0.8786, Val: 0.7360, Test: 0.7820\n",
      "Epoch: 037, Loss: 1.5734, Train: 0.8714, Val: 0.7480, Test: 0.7820\n",
      "Epoch: 038, Loss: 1.5669, Train: 0.8643, Val: 0.7520, Test: 0.7820\n",
      "Epoch: 039, Loss: 1.5371, Train: 0.8643, Val: 0.7640, Test: 0.7820\n",
      "Epoch: 040, Loss: 1.5337, Train: 0.8714, Val: 0.7780, Test: 0.7820\n",
      "Epoch: 041, Loss: 1.5206, Train: 0.8714, Val: 0.7800, Test: 0.7820\n",
      "Epoch: 042, Loss: 1.4847, Train: 0.8643, Val: 0.7800, Test: 0.7820\n",
      "Epoch: 043, Loss: 1.4695, Train: 0.8500, Val: 0.7860, Test: 0.7790\n",
      "Epoch: 044, Loss: 1.4637, Train: 0.8643, Val: 0.7880, Test: 0.7840\n",
      "Epoch: 045, Loss: 1.4496, Train: 0.8500, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 046, Loss: 1.4301, Train: 0.8500, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 047, Loss: 1.3931, Train: 0.8500, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 048, Loss: 1.4002, Train: 0.8571, Val: 0.7740, Test: 0.7840\n",
      "Epoch: 049, Loss: 1.3681, Train: 0.8500, Val: 0.7760, Test: 0.7840\n",
      "Epoch: 050, Loss: 1.3441, Train: 0.8571, Val: 0.7760, Test: 0.7840\n",
      "Epoch: 051, Loss: 1.3466, Train: 0.8571, Val: 0.7680, Test: 0.7840\n",
      "Epoch: 052, Loss: 1.2954, Train: 0.8714, Val: 0.7780, Test: 0.7840\n",
      "Epoch: 053, Loss: 1.2812, Train: 0.8857, Val: 0.7860, Test: 0.7840\n",
      "Epoch: 054, Loss: 1.2780, Train: 0.8786, Val: 0.7860, Test: 0.7840\n",
      "Epoch: 055, Loss: 1.2529, Train: 0.8786, Val: 0.7960, Test: 0.8030\n",
      "Epoch: 056, Loss: 1.2253, Train: 0.8857, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 057, Loss: 1.2283, Train: 0.8857, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 058, Loss: 1.2070, Train: 0.8714, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 059, Loss: 1.1502, Train: 0.8500, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 060, Loss: 1.1882, Train: 0.8500, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 061, Loss: 1.1326, Train: 0.8571, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 062, Loss: 1.1322, Train: 0.8571, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 063, Loss: 1.0985, Train: 0.8500, Val: 0.7820, Test: 0.8060\n",
      "Epoch: 064, Loss: 1.0886, Train: 0.8571, Val: 0.7820, Test: 0.8060\n",
      "Epoch: 065, Loss: 1.1089, Train: 0.8571, Val: 0.7840, Test: 0.8060\n",
      "Epoch: 066, Loss: 1.0680, Train: 0.8571, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 067, Loss: 1.0379, Train: 0.8643, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 068, Loss: 1.0343, Train: 0.8857, Val: 0.7840, Test: 0.8060\n",
      "Epoch: 069, Loss: 1.0059, Train: 0.9000, Val: 0.7840, Test: 0.8060\n",
      "Epoch: 070, Loss: 1.0023, Train: 0.9000, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 071, Loss: 0.9941, Train: 0.9071, Val: 0.8000, Test: 0.8060\n",
      "Epoch: 072, Loss: 1.0081, Train: 0.9000, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 073, Loss: 0.9885, Train: 0.9071, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 074, Loss: 0.8987, Train: 0.9000, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 075, Loss: 0.9466, Train: 0.9000, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 076, Loss: 0.9001, Train: 0.8929, Val: 0.7800, Test: 0.8060\n",
      "Epoch: 077, Loss: 0.9050, Train: 0.8714, Val: 0.7780, Test: 0.8060\n",
      "Epoch: 078, Loss: 0.8952, Train: 0.8714, Val: 0.7740, Test: 0.8060\n",
      "Epoch: 079, Loss: 0.8699, Train: 0.8714, Val: 0.7760, Test: 0.8060\n",
      "Epoch: 080, Loss: 0.8793, Train: 0.8786, Val: 0.7800, Test: 0.8060\n",
      "Epoch: 081, Loss: 0.8592, Train: 0.8929, Val: 0.7840, Test: 0.8060\n",
      "Epoch: 082, Loss: 0.8638, Train: 0.9071, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 083, Loss: 0.8328, Train: 0.9071, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 084, Loss: 0.8535, Train: 0.9143, Val: 0.8000, Test: 0.8060\n",
      "Epoch: 085, Loss: 0.8108, Train: 0.9143, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 086, Loss: 0.7907, Train: 0.9071, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 087, Loss: 0.8465, Train: 0.9000, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 088, Loss: 0.7771, Train: 0.9000, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 089, Loss: 0.7941, Train: 0.9000, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 090, Loss: 0.7692, Train: 0.9000, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 091, Loss: 0.7248, Train: 0.9214, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 092, Loss: 0.7198, Train: 0.9214, Val: 0.8000, Test: 0.8060\n",
      "Epoch: 093, Loss: 0.7475, Train: 0.9143, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 094, Loss: 0.7583, Train: 0.9143, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 095, Loss: 0.7450, Train: 0.9143, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 096, Loss: 0.7293, Train: 0.9143, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 097, Loss: 0.7646, Train: 0.9143, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 098, Loss: 0.7324, Train: 0.9143, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 099, Loss: 0.7258, Train: 0.9143, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 100, Loss: 0.6812, Train: 0.9214, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 101, Loss: 0.6919, Train: 0.9214, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 102, Loss: 0.6909, Train: 0.9214, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 103, Loss: 0.7154, Train: 0.9214, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 104, Loss: 0.6548, Train: 0.9143, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 105, Loss: 0.6809, Train: 0.9143, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 106, Loss: 0.6615, Train: 0.9143, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 107, Loss: 0.6698, Train: 0.9143, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 108, Loss: 0.6623, Train: 0.9214, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 109, Loss: 0.6326, Train: 0.9214, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 110, Loss: 0.6620, Train: 0.9214, Val: 0.8000, Test: 0.8060\n",
      "Epoch: 111, Loss: 0.6232, Train: 0.9214, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 112, Loss: 0.6365, Train: 0.9214, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 113, Loss: 0.6096, Train: 0.9286, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 114, Loss: 0.6200, Train: 0.9214, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 115, Loss: 0.5828, Train: 0.9214, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 116, Loss: 0.5892, Train: 0.9286, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 117, Loss: 0.5962, Train: 0.9286, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 118, Loss: 0.6113, Train: 0.9143, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 119, Loss: 0.6075, Train: 0.9214, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 120, Loss: 0.5762, Train: 0.9214, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 121, Loss: 0.5924, Train: 0.9214, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 122, Loss: 0.5673, Train: 0.9286, Val: 0.7880, Test: 0.8060\n",
      "Epoch: 123, Loss: 0.6043, Train: 0.9286, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 124, Loss: 0.5868, Train: 0.9357, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 125, Loss: 0.5714, Train: 0.9429, Val: 0.8020, Test: 0.8060\n",
      "Epoch: 126, Loss: 0.5812, Train: 0.9429, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 127, Loss: 0.5707, Train: 0.9357, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 128, Loss: 0.5491, Train: 0.9286, Val: 0.7940, Test: 0.8150\n",
      "Epoch: 129, Loss: 0.5560, Train: 0.9214, Val: 0.7960, Test: 0.8150\n",
      "Epoch: 130, Loss: 0.5217, Train: 0.9214, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 131, Loss: 0.5795, Train: 0.9286, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 132, Loss: 0.5350, Train: 0.9214, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 133, Loss: 0.5743, Train: 0.9286, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 134, Loss: 0.5362, Train: 0.9286, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 135, Loss: 0.5349, Train: 0.9286, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 136, Loss: 0.5588, Train: 0.9429, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 137, Loss: 0.5442, Train: 0.9429, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 138, Loss: 0.5110, Train: 0.9357, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 139, Loss: 0.5318, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 140, Loss: 0.5086, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 141, Loss: 0.5035, Train: 0.9357, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 142, Loss: 0.4724, Train: 0.9357, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 143, Loss: 0.5268, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 144, Loss: 0.4975, Train: 0.9429, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 145, Loss: 0.5304, Train: 0.9357, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 146, Loss: 0.4907, Train: 0.9357, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 147, Loss: 0.4839, Train: 0.9357, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 148, Loss: 0.4917, Train: 0.9357, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 149, Loss: 0.4993, Train: 0.9357, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 150, Loss: 0.4918, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 151, Loss: 0.5034, Train: 0.9357, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 152, Loss: 0.4858, Train: 0.9429, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 153, Loss: 0.4308, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 154, Loss: 0.4585, Train: 0.9357, Val: 0.7980, Test: 0.8150\n",
      "Epoch: 155, Loss: 0.4331, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 156, Loss: 0.4445, Train: 0.9357, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 157, Loss: 0.4676, Train: 0.9429, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 158, Loss: 0.4631, Train: 0.9429, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 159, Loss: 0.4575, Train: 0.9357, Val: 0.8040, Test: 0.8150\n",
      "Epoch: 160, Loss: 0.4628, Train: 0.9429, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 161, Loss: 0.4923, Train: 0.9429, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 162, Loss: 0.4599, Train: 0.9429, Val: 0.8060, Test: 0.8150\n",
      "Epoch: 163, Loss: 0.4759, Train: 0.9357, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 164, Loss: 0.4427, Train: 0.9429, Val: 0.8120, Test: 0.8170\n",
      "Epoch: 165, Loss: 0.4531, Train: 0.9429, Val: 0.8120, Test: 0.8170\n",
      "Epoch: 166, Loss: 0.4823, Train: 0.9429, Val: 0.8100, Test: 0.8170\n",
      "Epoch: 167, Loss: 0.4451, Train: 0.9429, Val: 0.8120, Test: 0.8170\n",
      "Epoch: 168, Loss: 0.4870, Train: 0.9429, Val: 0.8100, Test: 0.8170\n",
      "Epoch: 169, Loss: 0.4404, Train: 0.9357, Val: 0.8100, Test: 0.8170\n",
      "Epoch: 170, Loss: 0.4547, Train: 0.9429, Val: 0.8060, Test: 0.8170\n",
      "Epoch: 171, Loss: 0.4506, Train: 0.9429, Val: 0.8040, Test: 0.8170\n",
      "Epoch: 172, Loss: 0.4293, Train: 0.9429, Val: 0.8040, Test: 0.8170\n",
      "Epoch: 173, Loss: 0.4475, Train: 0.9429, Val: 0.8040, Test: 0.8170\n",
      "Epoch: 174, Loss: 0.4129, Train: 0.9429, Val: 0.8000, Test: 0.8170\n",
      "Epoch: 175, Loss: 0.4485, Train: 0.9571, Val: 0.8040, Test: 0.8170\n",
      "Epoch: 176, Loss: 0.4683, Train: 0.9429, Val: 0.8140, Test: 0.8160\n",
      "Epoch: 177, Loss: 0.3939, Train: 0.9429, Val: 0.8100, Test: 0.8160\n",
      "Epoch: 178, Loss: 0.4068, Train: 0.9500, Val: 0.8100, Test: 0.8160\n",
      "Epoch: 179, Loss: 0.4619, Train: 0.9429, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 180, Loss: 0.4509, Train: 0.9429, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 181, Loss: 0.4464, Train: 0.9429, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 182, Loss: 0.4279, Train: 0.9429, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 183, Loss: 0.4161, Train: 0.9500, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 184, Loss: 0.4318, Train: 0.9571, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 185, Loss: 0.4289, Train: 0.9571, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 186, Loss: 0.4248, Train: 0.9571, Val: 0.8100, Test: 0.8160\n",
      "Epoch: 187, Loss: 0.4023, Train: 0.9643, Val: 0.8140, Test: 0.8160\n",
      "Epoch: 188, Loss: 0.3923, Train: 0.9643, Val: 0.8120, Test: 0.8160\n",
      "Epoch: 189, Loss: 0.3943, Train: 0.9643, Val: 0.8120, Test: 0.8160\n",
      "Epoch: 190, Loss: 0.3826, Train: 0.9500, Val: 0.8080, Test: 0.8160\n",
      "Epoch: 191, Loss: 0.4394, Train: 0.9571, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 192, Loss: 0.3902, Train: 0.9500, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 193, Loss: 0.3751, Train: 0.9500, Val: 0.8060, Test: 0.8160\n",
      "Epoch: 194, Loss: 0.3863, Train: 0.9500, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 195, Loss: 0.4223, Train: 0.9500, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 196, Loss: 0.3728, Train: 0.9643, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 197, Loss: 0.3817, Train: 0.9643, Val: 0.8000, Test: 0.8160\n",
      "Epoch: 198, Loss: 0.3699, Train: 0.9643, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 199, Loss: 0.3922, Train: 0.9643, Val: 0.8040, Test: 0.8160\n",
      "Epoch: 200, Loss: 0.3875, Train: 0.9643, Val: 0.8080, Test: 0.8160\n"
     ]
    }
   ],
   "source": [
    "!python gcn.py --problem 3 \n",
    "# 이상하게 성능이 원래랑 같음 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57f4e5",
   "metadata": {},
   "source": [
    "### 4. Cora 데이터의 각 label별로 training data, val, test data의 수를 학인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d48fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: 140\n",
      "val data: 500\n",
      "test data: 1000\n"
     ]
    }
   ],
   "source": [
    "!python gcn.py --problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11beaf1e",
   "metadata": {},
   "source": [
    "### 5. training data의 edge list가 undirected인지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4f21e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is undirected: True\n"
     ]
    }
   ],
   "source": [
    "!python gcn.py --problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19073612",
   "metadata": {},
   "source": [
    "### 6. hyperparameter를 변경해 가며, 최고의 성능을 찾기 - 경쟁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5754a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d886ec8c",
   "metadata": {},
   "source": [
    "### 7. layer 하나를 더 쌓아 성능을 올리기 - 경쟁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3753b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "982e5af1",
   "metadata": {},
   "source": [
    "### C. PubMed에서 최고의 성적을 내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad29a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
